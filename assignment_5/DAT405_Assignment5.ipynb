{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.2 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Assignment5_Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "ad4bdff1320f12d81f077364a0ed0b61c72bf243b701352d6f35920f5f96452c"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 5: Reinforcement learning <br/>\r\n",
        "**DAT405 Introduction to Data Science and AI, 2021-2022, Study Period 1** <br/>\r\n",
        "**Due Date:**  Oct 4, 23:59<br/>\r\n",
        "**Authors:** Sidner Magnéli, Lukas Martinsson <br/>\r\n",
        "**Hours:** way to many :) \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "**What to submit**\r\n",
        "*   **The entire assignment should be submitted through the notebook. No separate file will be accepted.** You can submit either the notebook itself, or a public link to a Google Colab notebook<br/>\r\n",
        "\r\n",
        "*In the notebook:*\r\n",
        "*\tState your names and how many hours each person spent on the assignment.\r\n",
        "*\tThe solutions and answers to the theoretical and practical problems, including LaTeX math-mode equations, plots and tables etc.\r\n",
        "*\tAll plots/results should be visible such that the notebook does not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.<br/>\r\n",
        "\r\n",
        "*Before submitting:*\r\n",
        "*   Make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com."
      ],
      "metadata": {
        "id": "O73TxqjJH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-check**<br/>\r\n",
        "Is all the required information included? Have you answered all questions to the best of your ability? Anything else you can easily check? (details, terminology, arguments, clearly stated answers etc.?) Does your notebook run and can reproduce the results, plots and tables?\r\n",
        "\r\n",
        "**Grading**<br/>\r\n",
        "Grading will be based on a qualitative assessment of each assignment. It is important to:\r\n",
        "*\tPresent clear arguments\r\n",
        "*\tPresent the results in a pedagogical way\r\n",
        "*\tShow understanding of the topics (e.g, write a pseudocode) \r\n",
        "*\tGive correct solutions\r\n",
        "*\tMake sure that the code is well commented \r\n",
        "\r\n",
        "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (Assignment5_Reinforcement_Learning.ipynb) should be submitted with answers and code written in it. No separate file will be accepted.** \r\n"
      ],
      "metadata": {
        "id": "jDuY3qwbH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Primer"
      ],
      "metadata": {
        "id": "m_6obY12H7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\r\n",
        "### Decision Making\r\n",
        "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\r\n",
        "two parts. First, how do we learn about the world? This involves both the\r\n",
        "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\r\n",
        "currently know about the world, how should we decide what to do, taking into\r\n",
        "account future events and observations that may change our conclusions?\r\n",
        "Typically, this will involve creating long-term plans covering possible future\r\n",
        "eventualities. That is, when planning under uncertainty, we also need to take\r\n",
        "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\r\n",
        "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\r\n",
        "known to produce good results and experiment with something new is known\r\n",
        "as the **exploration-exploitation dilemma**.\r\n",
        "\r\n",
        "### The exploration-exploitation trade-off\r\n",
        "\r\n",
        "Consider the problem of selecting a restaurant to go to during a vacation. Lets say the\r\n",
        "best restaurant you have found so far was **Les Epinards**. The food there is\r\n",
        "usually to your taste and satisfactory. However, a well-known recommendations\r\n",
        "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\r\n",
        "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\r\n",
        "in which case you will regret going there. On the other hand, it could also be\r\n",
        "much better. What should you do?\r\n",
        "It all depends on how much information you have about either restaurant,\r\n",
        "and how many more days you’ll stay in town. If this is your last day, then it’s\r\n",
        "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\r\n",
        "Arm** to be significantly better. However, if you are going to stay there longer,\r\n",
        "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\r\n",
        "better food for the remaining time, while otherwise you will have missed only\r\n",
        "one good meal out of many, making the potential risk quite small."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\r\n"
      ],
      "metadata": {
        "id": "fC-SVg0ZH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\r\n",
        "\r\n",
        "\r\n",
        "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Decision Process"
      ],
      "metadata": {
        "id": "2zZ6HjxpH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\r\n",
        "\r\n",
        "* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\r\n",
        "* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\r\n",
        "* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\r\n",
        "* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\r\n",
        "* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\r\n",
        "\r\n",
        "At a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\r\n",
        "\r\n",
        "A *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \r\n",
        "\r\n",
        "$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \r\n",
        "\r\n",
        "where $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\r\n",
        "\r\n",
        "$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\r\n",
        "\r\n",
        "Now, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^{\\pi^*}(s)$ for all $s\\in S$. That is\r\n",
        "\r\n",
        "$$V^{\\pi^*}(s) \\geq V^\\pi(s), s\\in S$$\r\n",
        "\r\n",
        "The problem of finding the optimal policy is a _dynamic programming problem_. It turns out that a solution to the optimal policy problem in this context is the *Bellman equation*. The Bellman equation is given by\r\n",
        "\r\n",
        "$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\r\n",
        "\r\n",
        "Thus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\r\n",
        "\r\n",
        "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \r\n",
        "\r\n",
        "A major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \r\n"
      ],
      "metadata": {
        "id": "KurOZxYjH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1"
      ],
      "metadata": {
        "id": "0iUmTgzwH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first question we work with the deterministic MDP, no code is necessary in this part.\r\n",
        "\r\n",
        "Setup:\r\n",
        "\r\n",
        "* The agent starts in state **S**\r\n",
        "* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \r\n",
        "* Note, that you cannot move outside the grid, thus all actions are not available in every box.\r\n",
        "* When reaching **F**, the game ends (absorbing state).\r\n",
        "* The numbers in the boxes represent the rewards you receive when moving into that box. \r\n",
        "* Assume no discount in this model: $\\gamma = 1$\r\n",
        "\r\n",
        "The reward of a state $r(s=(x, y))$ is given by the values on the grid:\r\n",
        "    \r\n",
        "| | | |\r\n",
        "|----------|----------|---------|\r\n",
        "|-1 |1|**F**|\r\n",
        "|0|-1|1|  \r\n",
        "|-1 |0|-1|  \r\n",
        "|**S**|-1|1|\r\n",
        "\r\n",
        "Let $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$."
      ],
      "metadata": {
        "id": "QWPya78-H7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. `NESW` will make a circle."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal path is the one which maximizes the total rewards. One candidate for this might be the path going `EENNN`, with a final reward of zero. One could modify this path and still get the same total reward by instead going `EENNWNE`. In this case, one could argue the prior is more effective as it requires fewer steps to achieve the same final reward."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1b)** What is the optimal policy (i.e. the optimal action in each state)?\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list below shows the optimal actions of each state (matrix position) as follows `s`<sub>`i`</sub>` = (x,y)`. In the situation where multiple actions result in the same reward, the ones marked in bold marks the preferable action as these might lead to more value in the longer run.<br/> \r\n",
        "\r\n",
        "s<sub>1</sub> =  (0,0) ⇒ `N`, **`E`** <br/>\r\n",
        "s<sub>2</sub> =  (1,0) ⇒ `E`<br/>\r\n",
        "s<sub>3</sub> =  (2,0) ⇒ `W`, **`N`**<br/>\r\n",
        "s<sub>4</sub> =  (0,1) ⇒ `N`, `E`<br/>\r\n",
        "s<sub>5</sub> =  (1,1) ⇒ **`N`**, `S`, **`E`**, `W`<br/>\r\n",
        "s<sub>6</sub> =  (2,1) ⇒ **`N`**, `S`<br/>\r\n",
        "s<sub>7</sub> =  (0,2) ⇒ `N`, `E`<br/>\r\n",
        "s<sub>8</sub> =  (1,2) ⇒ `N`, `E`<br/>\r\n",
        "s<sub>9</sub> =  (2,2) ⇒ `N`<br/>\r\n",
        "s<sub>10</sub> = (0,3) ⇒ `E`<br/>\r\n",
        "s<sub>10</sub> = (1,3) ⇒ `E`<br/>\r\n",
        "s<sub>12</sub> = (2,3) = Absorbing state ⇒ no action"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1c)** What is expected total reward for the policy in 1b)?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected total reward for the policy above is given by calculating \r\n",
        "\r\n",
        "$$V^\\pi (S) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} S\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \r\n",
        "\r\n",
        "where $S$ is the starting point of the matrix. Hence, with this policy where the actions are not decided probabilistically meaning $p(s_{t+1} | a_t,s_t) = 1$, the above equation becomes\r\n",
        "\r\n",
        "$$V^\\pi (S) = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1})$$\r\n",
        "\r\n",
        "which when expanded becomes\r\n",
        "\r\n",
        "$$V^\\pi (S) = r(s_1,E,s_2) + r(s_2,E,s_3) + r(s_3,N,s_6) + r(s_6,N,s_9) + r(s_9,N,s_12)$$\r\n",
        "$$ = (-1) + 1 + (-1) + 1 + 0 = 0$$"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration"
      ],
      "metadata": {
        "id": "ZyQ7IatcH7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation. The process of repeated application of the Bellman equation what we here call the _value iteration_ algorithm."
      ],
      "metadata": {
        "id": "5NfqElM_H7e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value iteration algorithm practically procedes as follows:\r\n",
        "\r\n",
        "```\r\n",
        "epsilon is a small value, threshold\r\n",
        "for x from i to infinity \r\n",
        "do\r\n",
        "    for each state s\r\n",
        "    do\r\n",
        "        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\r\n",
        "    end\r\n",
        "    if  |V_k[s]-V_k-1[s]| < epsilon for all s\r\n",
        "        for each state s,\r\n",
        "        do\r\n",
        "            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\r\n",
        "            return π, V_k \r\n",
        "        end\r\n",
        "end\r\n",
        "\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "4qbn4HjqR2fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state, that is, we stay on the grid), and remain still 20 percent of the time. We will use a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \r\n",
        "\r\n",
        "| | | |  \r\n",
        "|----------|----------|---------|  \r\n",
        "|0|0|0|\r\n",
        "|0|10|0|  \r\n",
        "|0|0|0|  \r\n",
        "\r\n",
        "\r\n",
        "**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\r\n",
        "\r\n",
        "| | | |  \r\n",
        "|----------|----------|---------|  \r\n",
        "|0|8|0|\r\n",
        "|8|2|8|  \r\n",
        "|0|8|0|  \r\n",
        "  \r\n",
        "**Iteration 2**:  \r\n",
        "  \r\n",
        "Staring with cell (0,0) (lower left corner): We find the expected value of each move:  \r\n",
        "Action **S**: 0  \r\n",
        "Action **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \r\n",
        "Action **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \r\n",
        "Action **W**: 0\r\n",
        "\r\n",
        "Hence any action between **E** and **N** would be best at this stage.\r\n",
        "\r\n",
        "Similarly for cell (1,0):\r\n",
        "\r\n",
        "Action **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \r\n",
        "\r\n",
        "Similar calculations for remaining cells give us:\r\n",
        "\r\n",
        "| | | |  \r\n",
        "|----------|----------|---------|  \r\n",
        "|5.76|10.88|5.76|\r\n",
        "|10.88|8.12|10.88|  \r\n",
        "|5.76|10.88|5.76|  \r\n"
      ],
      "metadata": {
        "id": "K7hOzat7H7e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2"
      ],
      "metadata": {
        "id": "ccoMLc71H7e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2a)** Implement the value iteration algorithm just described here in python, and show the converging optimal value function and the optimal policy for the above 3x3 grid. Hint: use the pseudo-code above as a starting point, but be sure to explain what every line does."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# imports and global configuration\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# check if graphviz is installed\r\n",
        "try:\r\n",
        "    from graphviz import Digraph\r\n",
        "    has_graphviz = True\r\n",
        "except ImportError:\r\n",
        "    has_graphviz = False"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by implementing a **Markov Decision Process** class to ease implementation of the **Value Iteration** algoritm."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "class MDP:\r\n",
        "  def __init__(self, transition_probs, rewards, initial_state=None):\r\n",
        "    \"\"\"\r\n",
        "    Defines an MDP.\r\n",
        "    :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\r\n",
        "        A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\r\n",
        "        For each state and action, probabilities of next states should sum to 1\r\n",
        "        If a state has no actions available, it is considered terminal\r\n",
        "    :param rewards: rewards[s][a][s_next] = r(s,a,s')\r\n",
        "        A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\r\n",
        "        The reward for anything not mentioned here is zero.\r\n",
        "    :param get_initial_state: a state where agent starts or a callable() -> state\r\n",
        "        By default, picks initial state at random.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # validate transition probability dict\r\n",
        "    assert None not in transition_probs, \"please do not use None as a state identifier.\"\r\n",
        "    for state in transition_probs:\r\n",
        "      assert isinstance(transition_probs[state], dict), \\\r\n",
        "        \"transition_probs for %s should be a dictionary, not %s\" % (\r\n",
        "          state, type(transition_probs[state]))\r\n",
        "      for action in transition_probs[state]:\r\n",
        "        assert isinstance(transition_probs[state][action], dict), \\\r\n",
        "          \"transition_probs for %s, %s should be a a dictionary, not %s\" % (\r\n",
        "            state, action, type(transition_probs[state][action]))\r\n",
        "        next_state_probs = transition_probs[state][action]\r\n",
        "        assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\r\n",
        "        sum_probs = sum(next_state_probs.values())\r\n",
        "        assert abs(sum_probs - 1) <= 1e-10, \\\r\n",
        "          \"next state probabilities for state %s action %s add up to %f (should be 1)\" % (\r\n",
        "            state, action, sum_probs)\r\n",
        "\r\n",
        "    # validate reward function\r\n",
        "    assert None not in rewards, \"please do not use None as an action identifier.\"\r\n",
        "    for state in rewards:\r\n",
        "      assert isinstance(rewards[state], dict), \\\r\n",
        "        \"rewards for %s should be a dictionary but is instead %s\" % (\r\n",
        "          state, type(rewards[state]))\r\n",
        "      for action in rewards[state]:\r\n",
        "        assert isinstance(rewards[state][action], dict), \\\r\n",
        "          \"rewards for %s, %s should be a a dictionary but is instead %s\" % (\r\n",
        "            state, action, type(rewards[state][action]))\r\n",
        "\r\n",
        "    self._transition_probs = transition_probs\r\n",
        "    self._rewards = rewards\r\n",
        "    self.n_states = len(transition_probs)\r\n",
        "    \r\n",
        "    # set initial state\r\n",
        "    if initial_state in self._transition_probs:\r\n",
        "      self._current_state = initial_state\r\n",
        "    else:\r\n",
        "      raise ValueError(\"initial state %s should be either a state or a function() -> state\" % initial_state)  \r\n",
        "\r\n",
        "  def get_states(self):\r\n",
        "    \"\"\" return a tuple of all possible states \"\"\"\r\n",
        "    return tuple(self._transition_probs.keys())\r\n",
        "\r\n",
        "  def get_actions(self, state):\r\n",
        "    \"\"\" return a tuple of possible actions in a given state \"\"\"\r\n",
        "    return tuple(self._transition_probs.get(state, {}).keys())\r\n",
        "\r\n",
        "  def get_next_states(self, state, action):\r\n",
        "    \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\r\n",
        "    assert action in self.get_actions(state)\r\n",
        "    return self._transition_probs[state][action]\r\n",
        "\r\n",
        "  def get_transition_prob(self, state, action, next_state):\r\n",
        "    \"\"\" return P(next_state | state, action) \"\"\"\r\n",
        "    return self.get_next_states(state, action).get(next_state, 0.0)\r\n",
        "\r\n",
        "  def get_reward(self, state, action, next_state):\r\n",
        "    \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\r\n",
        "    assert action in self.get_actions(state), \"cannot do action %s from state %s\" % (action, state)\r\n",
        "    return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\r\n",
        "    "
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper methods"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def plot_graph(mdp, graph_size=\"10,10\", s_node_size=\"1,5\", a_node_size=\"0,5\", rankdir=\"LR\",):\r\n",
        "  \"\"\"\r\n",
        "  Function for pretty drawing MDP graph with graphviz library,\r\n",
        "  as it makes it easier to visualize the graph.\r\n",
        "  \r\n",
        "  Requirements:\r\n",
        "  graphviz : https://www.graphviz.org/\r\n",
        "\r\n",
        "  :param mdp:\r\n",
        "  :param graph_size: size of graph plot\r\n",
        "  :param s_node_size: size of state nodes\r\n",
        "  :param a_node_size: size of action nodes\r\n",
        "  :param rankdir: order for drawing\r\n",
        "  \"\"\"\r\n",
        "  s_node_attrs = {\r\n",
        "      \"shape\": \"doublecircle\",\r\n",
        "      \"color\": \"#85ff75\",\r\n",
        "      \"style\": \"filled\",\r\n",
        "      \"width\": str(s_node_size),\r\n",
        "      \"height\": str(s_node_size),\r\n",
        "      \"fontname\": \"Arial\",\r\n",
        "      \"fontsize\": \"24\",\r\n",
        "  }\r\n",
        "\r\n",
        "  a_node_attrs = {\r\n",
        "      \"shape\": \"circle\",\r\n",
        "      \"color\": \"lightpink\",\r\n",
        "      \"style\": \"filled\",\r\n",
        "      \"width\": str(a_node_size),\r\n",
        "      \"height\": str(a_node_size),\r\n",
        "      \"fontname\": \"Arial\",\r\n",
        "      \"fontsize\": \"20\",\r\n",
        "  }\r\n",
        "\r\n",
        "  s_a_edge_attrs = {\"style\": \"bold\", \"color\": \"red\", \"ratio\": \"auto\"}\r\n",
        "\r\n",
        "  a_s_edge_attrs = {\r\n",
        "      \"style\": \"dashed\",\r\n",
        "      \"color\": \"blue\",\r\n",
        "      \"ratio\": \"auto\",\r\n",
        "      \"fontname\": \"Arial\",\r\n",
        "      \"fontsize\": \"16\",\r\n",
        "  }\r\n",
        "\r\n",
        "  graph = Digraph(name=\"MDP\")\r\n",
        "  graph.attr(rankdir=rankdir, size=graph_size)\r\n",
        "  for state_node in mdp._transition_probs:\r\n",
        "    graph.node(str(state_node), **s_node_attrs)\r\n",
        "\r\n",
        "    for posible_action in mdp.get_actions(state_node):\r\n",
        "      action_node = str(state_node) + \"-\" + posible_action\r\n",
        "      graph.node(action_node, label=str(posible_action), **a_node_attrs)\r\n",
        "      graph.edge(str(state_node), str(state_node) + \"-\" + posible_action, **s_a_edge_attrs)\r\n",
        "\r\n",
        "      for posible_next_state in mdp.get_next_states(state_node, posible_action):\r\n",
        "        probability = mdp.get_transition_prob(\r\n",
        "          state_node, posible_action, posible_next_state\r\n",
        "        )\r\n",
        "        reward = mdp.get_reward(state_node, posible_action, posible_next_state)\r\n",
        "\r\n",
        "        if reward != 0:\r\n",
        "          label_a_s_edge = (\r\n",
        "              \"p = \" + str(probability) + \"  \" + \"\\nreward =\" + str(reward)\r\n",
        "          )\r\n",
        "        else:\r\n",
        "          label_a_s_edge = \"p = \" + str(probability)\r\n",
        "\r\n",
        "        graph.edge(\r\n",
        "          action_node,\r\n",
        "          str(posible_next_state),\r\n",
        "          label=label_a_s_edge,\r\n",
        "          **a_s_edge_attrs\r\n",
        "        )\r\n",
        "  return graph\r\n",
        "\r\n",
        "def get_action_value(mdp, state_values, state, action, discount):\r\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\r\n",
        "    Q = [mdp.get_transition_prob(state, action, s) * (mdp.get_reward(state, action, s) + discount * state_values[s]) for s in state_values]\r\n",
        "    return sum(Q)\r\n",
        "\r\n",
        "def get_new_state_value(mdp, state_values, state, discount):\r\n",
        "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\r\n",
        "    Q = [get_action_value(mdp, state_values, state, a, discount) for a in mdp.get_actions(state)]\r\n",
        "    return max(Q)\r\n",
        "\r\n",
        "def render(env, state_values):\r\n",
        "  \"\"\" render 2d array based on a state-value dictionary \"\"\"\r\n",
        "  (rows, cols) = env.shape\r\n",
        "  values = list(new_state_values.values())\r\n",
        "  for idy, row in enumerate(env):\r\n",
        "    for idx in range(0,cols):\r\n",
        "      env[rows-(idy+1)][idx] = round(values[(idy*cols)+idx], 4)\r\n",
        "  print(env, end='\\n\\n')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the **Markov Decision Process** through it's **transition probabilities** and **reward function**.  "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# define transition probabilities according to the description above.\r\n",
        "transition_probs = {\r\n",
        "  1: {\r\n",
        "    \"N\": {4: 0.8, 1: 0.2}, \r\n",
        "    \"E\": {2: 0.8, 1: 0.2}\r\n",
        "  },\r\n",
        "  2: {\r\n",
        "    \"N\": {5: 0.8, 2: 0.2}, \r\n",
        "    \"E\": {3: 0.8, 2: 0.2}, \r\n",
        "    \"W\": {1: 0.8, 2: 0.2}\r\n",
        "  },\r\n",
        "  3: {\r\n",
        "    \"N\": {6: 0.8, 3: 0.2}, \r\n",
        "    \"W\": {2: 0.8, 3: 0.2}\r\n",
        "  },\r\n",
        "  4: {\r\n",
        "    \"N\": {7: 0.8, 4: 0.2}, \r\n",
        "    \"E\": {5: 0.8, 4: 0.2}, \r\n",
        "    \"S\": {1: 0.8, 4: 0.2}\r\n",
        "  },\r\n",
        "  5: {\r\n",
        "    \"N\": {8: 0.8, 5: 0.2},\r\n",
        "    \"E\": {6: 0.8, 5: 0.2},\r\n",
        "    \"S\": {2: 0.8, 5: 0.2},\r\n",
        "    \"W\": {4: 0.8, 5: 0.2},\r\n",
        "  },\r\n",
        "  6: {\r\n",
        "    \"N\": {9: 0.8, 6: 0.2}, \r\n",
        "    \"S\": {3: 0.8, 6: 0.2}, \r\n",
        "    \"W\": {5: 0.8, 6: 0.2}\r\n",
        "  },\r\n",
        "  7: {\r\n",
        "    \"E\": {8: 0.8, 7: 0.2}, \r\n",
        "    \"S\": {4: 0.8, 7: 0.2}\r\n",
        "  },\r\n",
        "  8: {\r\n",
        "    \"E\": {9: 0.8, 8: 0.2}, \r\n",
        "    \"S\": {5: 0.8, 8: 0.2}, \r\n",
        "    \"W\": {7: 0.8, 8: 0.2}\r\n",
        "  },\r\n",
        "  9: {\r\n",
        "    \"S\": {6: 0.8, 9: 0.2}, \r\n",
        "    \"W\": {8: 0.8, 9: 0.2}\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "# define reward function / mapping\r\n",
        "rewards = {\r\n",
        "  1: {\r\n",
        "    \"N\": {4: 0, 1: 0}, \r\n",
        "    \"E\": {2: 0, 1: 0}\r\n",
        "  },\r\n",
        "  2: {\r\n",
        "    \"N\": {5: 10, 2: 0}, \r\n",
        "    \"E\": {3: 0, 2: 0}, \r\n",
        "    \"W\": {1: 0, 2: 0}\r\n",
        "  },\r\n",
        "  3: {\r\n",
        "    \"N\": {6: 0, 3: 0}, \r\n",
        "    \"W\": {2: 0, 3: 0}\r\n",
        "  },\r\n",
        "  4: {\r\n",
        "    \"N\": {7: 0, 4: 0}, \r\n",
        "    \"E\": {5: 10, 4: 0}, \r\n",
        "    \"S\": {1: 0, 4: 0}\r\n",
        "  },\r\n",
        "  5: {\r\n",
        "    \"N\": {8: 0, 5: 10}, \r\n",
        "    \"E\": {6: 0, 5: 10}, \r\n",
        "    \"S\": {2: 0, 5: 10}, \r\n",
        "    \"W\": {4: 0, 5: 10}\r\n",
        "  },\r\n",
        "  6: {\r\n",
        "    \"N\": {9: 0, 6: 0}, \r\n",
        "    \"S\": {3: 0, 6: 0}, \r\n",
        "    \"W\": {5: 10, 6: 0}\r\n",
        "  },\r\n",
        "  7: {\r\n",
        "    \"E\": {8: 0, 7: 0}, \r\n",
        "    \"S\": {4: 0, 7: 0}\r\n",
        "  },\r\n",
        "  8: {\r\n",
        "    \"E\": {9: 0, 8: 0}, \r\n",
        "    \"S\": {5: 10, 8: 0}, \r\n",
        "    \"W\": {7: 0, 8: 0}\r\n",
        "  },\r\n",
        "  9: {\r\n",
        "    \"S\": {6: 0, 9: 0}, \r\n",
        "    \"W\": {8: 0, 9: 0}\r\n",
        "  }\r\n",
        "}\r\n",
        "\r\n",
        "mdp = MDP(transition_probs, rewards, initial_state=1)\r\n",
        "\r\n",
        "from IPython.display import display\r\n",
        "if has_graphviz:\r\n",
        "    display(plot_graph(mdp, graph_size='15'))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: MDP Pages: 1 -->\n<svg width=\"1080pt\" height=\"239pt\"\n viewBox=\"0.00 0.00 1080.00 239.35\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(.3892 .3892) rotate(0) translate(4 610.9793)\">\n<title>MDP</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-610.9793 2770.9209,-610.9793 2770.9209,4 -4,4\"/>\n<!-- 1 -->\n<g id=\"node1\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"40\" cy=\"-180\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"40\" cy=\"-180\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-173.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">1</text>\n</g>\n<!-- 1&#45;N -->\n<g id=\"node2\" class=\"node\">\n<title>1&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"188.6274\" cy=\"-230\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"188.6274\" y=\"-225\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 1&#45;&gt;1&#45;N -->\n<g id=\"edge1\" class=\"edge\">\n<title>1&#45;&gt;1&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M72.5536,-203.3579C80.5593,-208.1971 89.3303,-212.7922 98,-216 116.3001,-222.7711 137.8503,-226.2677 155.344,-228.0731\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"155.3751,-231.5898 165.6467,-228.9929 155.9976,-224.6175 155.3751,-231.5898\"/>\n</g>\n<!-- 1&#45;E -->\n<g id=\"node4\" class=\"node\">\n<title>1&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"188.6274\" cy=\"-131\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"188.6274\" y=\"-126\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 1&#45;&gt;1&#45;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>1&#45;&gt;1&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M78.9839,-170.8438C99.7413,-165.5516 125.5501,-158.3018 148,-150 152.0259,-148.5113 156.2018,-146.7827 160.2764,-144.9866\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"161.8582,-148.1116 169.4819,-140.7542 158.9341,-141.7516 161.8582,-148.1116\"/>\n</g>\n<!-- 1&#45;N&#45;&gt;1 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;N&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M173.6441,-212.7918C166.5845,-205.8082 157.6072,-198.3712 148,-194 130.14,-185.8738 109.0114,-182.0355 90.1996,-180.3439\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"90.2479,-176.8384 80.0224,-179.6114 89.7453,-183.8203 90.2479,-176.8384\"/>\n<text text-anchor=\"middle\" x=\"123\" y=\"-199.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 4 -->\n<g id=\"node3\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"337.2548\" cy=\"-416\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"337.2548\" cy=\"-416\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"337.2548\" y=\"-409.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">4</text>\n</g>\n<!-- 1&#45;N&#45;&gt;4 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;N&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M202.8784,-247.8345C226.4161,-277.2907 273.9648,-336.7956 305.7165,-376.5313\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"303.261,-379.0651 312.2378,-384.6924 308.7295,-374.6953 303.261,-379.0651\"/>\n<text text-anchor=\"middle\" x=\"254.2548\" y=\"-345.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 4&#45;N -->\n<g id=\"node14\" class=\"node\">\n<title>4&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"485.8823\" cy=\"-449\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"485.8823\" y=\"-444\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 4&#45;&gt;4&#45;N -->\n<g id=\"edge22\" class=\"edge\">\n<title>4&#45;&gt;4&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M377.3543,-418.2409C397.9598,-420.1306 423.2887,-423.6079 445.2548,-430 448.909,-431.0633 452.6487,-432.4132 456.3081,-433.9014\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"455.1329,-437.2089 465.6933,-438.0622 457.9699,-430.8096 455.1329,-437.2089\"/>\n</g>\n<!-- 4&#45;E -->\n<g id=\"node16\" class=\"node\">\n<title>4&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"485.8823\" cy=\"-321\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"485.8823\" y=\"-316\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 4&#45;&gt;4&#45;E -->\n<g id=\"edge25\" class=\"edge\">\n<title>4&#45;&gt;4&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M367.316,-389.0712C376.0381,-381.8328 385.7737,-374.2781 395.2548,-368 406.3105,-360.6793 435.4018,-345.8605 457.6693,-334.8093\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"459.3071,-337.904 466.7223,-330.3366 456.2065,-331.6282 459.3071,-337.904\"/>\n</g>\n<!-- 4&#45;S -->\n<g id=\"node17\" class=\"node\">\n<title>4&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"485.8823\" cy=\"-583\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"485.8823\" y=\"-578\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 4&#45;&gt;4&#45;S -->\n<g id=\"edge28\" class=\"edge\">\n<title>4&#45;&gt;4&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M348.6758,-454.3897C357.8809,-480.0226 372.996,-513.1045 395.2548,-536 400.0751,-540.9582 432.6987,-557.338 457.4353,-569.3712\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"455.9294,-572.5307 466.4553,-573.7373 458.9793,-566.23 455.9294,-572.5307\"/>\n</g>\n<!-- 1&#45;E&#45;&gt;1 -->\n<g id=\"edge6\" class=\"edge\">\n<title>1&#45;E&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M167.714,-125.8245C148.8393,-122.1815 120.6425,-119.336 98,-128 89.2808,-131.3363 81.0466,-136.6544 73.6698,-142.6429\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"71.2369,-140.121 66.0194,-149.3421 75.8484,-145.3874 71.2369,-140.121\"/>\n<text text-anchor=\"middle\" x=\"123\" y=\"-133.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 2 -->\n<g id=\"node5\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"2227.926\" cy=\"-148\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"2227.926\" cy=\"-148\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"2227.926\" y=\"-141.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">2</text>\n</g>\n<!-- 1&#45;E&#45;&gt;2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;E&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M205.3561,-117.763C231.7659,-98.2371 285.4585,-64 337.2548,-64 337.2548,-64 337.2548,-64 1895.4285,-64 2000.1972,-64 2118.0338,-103.6881 2181.4725,-128.5035\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2180.217,-131.7707 2190.8031,-132.2002 2182.7954,-125.2629 2180.217,-131.7707\"/>\n<text text-anchor=\"middle\" x=\"1147.4335\" y=\"-69.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 2&#45;N -->\n<g id=\"node6\" class=\"node\">\n<title>2&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2378.6747\" cy=\"-266\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"2378.6747\" y=\"-261\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 2&#45;&gt;2&#45;N -->\n<g id=\"edge7\" class=\"edge\">\n<title>2&#45;&gt;2&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2255.1154,-177.5774C2264.4801,-187.1013 2275.2943,-197.4061 2285.926,-206 2306.1772,-222.3696 2331.0381,-238.2536 2349.9217,-249.5621\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2348.2754,-252.6546 2358.6653,-254.7281 2351.8361,-246.6279 2348.2754,-252.6546\"/>\n</g>\n<!-- 2&#45;E -->\n<g id=\"node8\" class=\"node\">\n<title>2&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2378.6747\" cy=\"-193\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"2378.6747\" y=\"-188\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 2&#45;&gt;2&#45;E -->\n<g id=\"edge10\" class=\"edge\">\n<title>2&#45;&gt;2&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2267.4068,-155.6971C2288.0736,-160.1778 2313.646,-166.4372 2335.926,-174 2340.5837,-175.581 2345.4301,-177.4774 2350.1152,-179.4513\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2348.8311,-182.7101 2359.3937,-183.5361 2351.6516,-176.3035 2348.8311,-182.7101\"/>\n</g>\n<!-- 2&#45;W -->\n<g id=\"node10\" class=\"node\">\n<title>2&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2378.6747\" cy=\"-122\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"2378.6747\" y=\"-117\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 2&#45;&gt;2&#45;W -->\n<g id=\"edge13\" class=\"edge\">\n<title>2&#45;&gt;2&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2258.6134,-121.8158C2266.957,-116.1578 2276.3494,-110.9821 2285.926,-108 2305.252,-101.9821 2327.676,-105.1084 2345.6337,-109.9179\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2344.7852,-113.3168 2355.3675,-112.8013 2346.7734,-106.6051 2344.7852,-113.3168\"/>\n</g>\n<!-- 2&#45;N&#45;&gt;2 -->\n<g id=\"edge9\" class=\"edge\">\n<title>2&#45;N&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2356.088,-263.6311C2335.9746,-260.5547 2306.613,-253.5885 2285.926,-238 2270.6443,-226.4846 2258.2745,-209.6784 2248.9456,-193.698\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2251.996,-191.9817 2244.0765,-184.9438 2245.8786,-195.3842 2251.996,-191.9817\"/>\n<text text-anchor=\"middle\" x=\"2310.926\" y=\"-265.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 5 -->\n<g id=\"node7\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"2560.4234\" cy=\"-336\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"2560.4234\" cy=\"-336\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"2560.4234\" y=\"-329.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">5</text>\n</g>\n<!-- 2&#45;N&#45;&gt;5 -->\n<g id=\"edge8\" class=\"edge\">\n<title>2&#45;N&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2400.9127,-260.824C2426.6772,-256.001 2469.7659,-251.6279 2502.4234,-267 2514.7218,-272.7889 2525.3517,-282.5929 2534.0294,-293.0209\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2531.3791,-295.3143 2540.2801,-301.0607 2536.9054,-291.0177 2531.3791,-295.3143\"/>\n<text text-anchor=\"middle\" x=\"2461.9234\" y=\"-290.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8 &#160;</text>\n<text text-anchor=\"middle\" x=\"2461.9234\" y=\"-272.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 5&#45;N -->\n<g id=\"node18\" class=\"node\">\n<title>5&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2742.1722\" cy=\"-404\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"2742.1722\" y=\"-399\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 5&#45;&gt;5&#45;N -->\n<g id=\"edge31\" class=\"edge\">\n<title>5&#45;&gt;5&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2586.3006,-366.7284C2595.5011,-375.6928 2606.5495,-384.5375 2618.4234,-390 2647.2539,-403.2633 2683.7044,-405.9198 2709.5595,-405.7723\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2709.6672,-409.2709 2719.5953,-405.572 2709.5275,-402.2723 2709.6672,-409.2709\"/>\n</g>\n<!-- 5&#45;E -->\n<g id=\"node20\" class=\"node\">\n<title>5&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2742.1722\" cy=\"-25\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"2742.1722\" y=\"-20\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 5&#45;&gt;5&#45;E -->\n<g id=\"edge34\" class=\"edge\">\n<title>5&#45;&gt;5&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2562.5364,-296.0071C2567.2664,-223.1647 2581.3594,-73.2045 2618.4234,-39 2643.0796,-16.2462 2683.3131,-15.9331 2711.1801,-19.2159\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2710.8095,-22.698 2721.1967,-20.6109 2711.7751,-15.7649 2710.8095,-22.698\"/>\n</g>\n<!-- 5&#45;S -->\n<g id=\"node21\" class=\"node\">\n<title>5&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2742.1722\" cy=\"-269\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"2742.1722\" y=\"-264\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 5&#45;&gt;5&#45;S -->\n<g id=\"edge37\" class=\"edge\">\n<title>5&#45;&gt;5&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2600.5172,-332.6894C2629.2749,-329.069 2668.2788,-321.5472 2699.4234,-306 2706.8333,-302.301 2714.0025,-297.0211 2720.2848,-291.6041\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2722.7204,-294.1194 2727.7193,-284.778 2717.9861,-288.9631 2722.7204,-294.1194\"/>\n</g>\n<!-- 5&#45;W -->\n<g id=\"node22\" class=\"node\">\n<title>5&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2742.1722\" cy=\"-523\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"2742.1722\" y=\"-518\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 5&#45;&gt;5&#45;W -->\n<g id=\"edge40\" class=\"edge\">\n<title>5&#45;&gt;5&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M2583.7782,-368.7632C2593.913,-382.2812 2606.2368,-397.8325 2618.4234,-411 2649.3262,-444.39 2688.8711,-478.952 2714.8335,-500.6887\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2712.8305,-503.5751 2722.7574,-507.2777 2717.3061,-498.1928 2712.8305,-503.5751\"/>\n</g>\n<!-- 2&#45;E&#45;&gt;2 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;E&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2357.8124,-188.9528C2338.8598,-185.064 2310.2336,-178.6583 2285.926,-171 2282.1969,-169.8251 2278.3664,-168.5243 2274.5372,-167.1565\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2275.6731,-163.8449 2265.0801,-163.6518 2273.2407,-170.4087 2275.6731,-163.8449\"/>\n<text text-anchor=\"middle\" x=\"2310.926\" y=\"-189.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 3 -->\n<g id=\"node9\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"1895.4285\" cy=\"-248\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"1895.4285\" cy=\"-248\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"1895.4285\" y=\"-241.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">3</text>\n</g>\n<!-- 2&#45;E&#45;&gt;3 -->\n<g id=\"edge11\" class=\"edge\">\n<title>2&#45;E&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2358.4002,-199.8418C2351.3469,-202.041 2343.3437,-204.336 2335.926,-206 2300.036,-214.0509 2105.6695,-246.7126 2003.4285,-252 1984.5443,-252.9766 1963.7906,-252.5643 1945.5662,-251.7054\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1945.6169,-248.2033 1935.4465,-251.1718 1945.2483,-255.1936 1945.6169,-248.2033\"/>\n<text text-anchor=\"middle\" x=\"2129.426\" y=\"-249.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 3&#45;N -->\n<g id=\"node11\" class=\"node\">\n<title>3&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2046.1772\" cy=\"-213\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"2046.1772\" y=\"-208\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 3&#45;&gt;3&#45;N -->\n<g id=\"edge16\" class=\"edge\">\n<title>3&#45;&gt;3&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1935.4789,-244.6811C1956.0714,-242.3211 1981.4035,-238.3968 2003.4285,-232 2007.6402,-230.7768 2011.9771,-229.2242 2016.1991,-227.5357\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2017.9234,-230.6042 2025.7288,-223.4399 2015.1593,-224.173 2017.9234,-230.6042\"/>\n</g>\n<!-- 3&#45;W -->\n<g id=\"node13\" class=\"node\">\n<title>3&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"2046.1772\" cy=\"-148\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"2046.1772\" y=\"-143\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 3&#45;&gt;3&#45;W -->\n<g id=\"edge19\" class=\"edge\">\n<title>3&#45;&gt;3&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1903.0504,-208.485C1910.6896,-180.7198 1925.4967,-145.5229 1953.4285,-128 1972.0502,-116.3177 1996.4618,-122.2505 2015.5662,-130.6076\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"2014.208,-133.8373 2024.7413,-134.9777 2017.2181,-127.5176 2014.208,-133.8373\"/>\n</g>\n<!-- 2&#45;W&#45;&gt;1 -->\n<g id=\"edge14\" class=\"edge\">\n<title>2&#45;W&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2356.9084,-110.1014C2350.3049,-106.8236 2342.9393,-103.4891 2335.926,-101 2182.3008,-46.4765 2141.4424,-19 1978.4285,-19 188.6274,-19 188.6274,-19 188.6274,-19 125.8603,-19 82.261,-85.8827 59.1958,-133.4321\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"55.9453,-132.1199 54.857,-142.6587 62.2798,-135.0988 55.9453,-132.1199\"/>\n<text text-anchor=\"middle\" x=\"1147.4335\" y=\"-24.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 2&#45;W&#45;&gt;2 -->\n<g id=\"edge15\" class=\"edge\">\n<title>2&#45;W&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2353.6377,-121.8176C2334.837,-122.1142 2308.5479,-123.4536 2285.926,-128 2282.32,-128.7247 2278.6316,-129.6178 2274.9486,-130.6206\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2273.8636,-127.2916 2265.2722,-133.4914 2275.8547,-134.0025 2273.8636,-127.2916\"/>\n<text text-anchor=\"middle\" x=\"2310.926\" y=\"-133.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 3&#45;N&#45;&gt;3 -->\n<g id=\"edge18\" class=\"edge\">\n<title>3&#45;N&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2023.9325,-208.3892C2004.8197,-205.3373 1976.7397,-203.0436 1953.4285,-210 1947.3724,-211.8072 1941.3609,-214.4714 1935.6305,-217.565\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1933.727,-214.6241 1926.8908,-222.7184 1937.2826,-220.6539 1933.727,-214.6241\"/>\n<text text-anchor=\"middle\" x=\"1978.4285\" y=\"-215.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 6 -->\n<g id=\"node12\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"1562.931\" cy=\"-278\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"1562.931\" cy=\"-278\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"1562.931\" y=\"-271.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">6</text>\n</g>\n<!-- 3&#45;N&#45;&gt;6 -->\n<g id=\"edge17\" class=\"edge\">\n<title>3&#45;N&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2024.1692,-206.4054C2017.559,-204.6783 2010.2502,-203.0169 2003.4285,-202 1973.5075,-197.5399 1965.6728,-199.6668 1935.4285,-199 1825.6702,-196.58 1797.8166,-200.9761 1688.931,-215 1658.5667,-218.9108 1648.5561,-212.8038 1620.931,-226 1612.5967,-229.9812 1604.5813,-235.5392 1597.3077,-241.5493\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1594.939,-238.9714 1589.7263,-248.1952 1599.5533,-244.2352 1594.939,-238.9714\"/>\n<text text-anchor=\"middle\" x=\"1796.9285\" y=\"-212.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 6&#45;N -->\n<g id=\"node23\" class=\"node\">\n<title>6&#45;N</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1713.6798\" cy=\"-375\" rx=\"22.7556\" ry=\"22.7556\"/>\n<text text-anchor=\"middle\" x=\"1713.6798\" y=\"-370\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">N</text>\n</g>\n<!-- 6&#45;&gt;6&#45;N -->\n<g id=\"edge43\" class=\"edge\">\n<title>6&#45;&gt;6&#45;N</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1596.6175,-300.1799C1604.5766,-305.3899 1613.0425,-310.9071 1620.931,-316 1642.7008,-330.0548 1667.4694,-345.7982 1685.9315,-357.4851\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1684.1249,-360.4838 1694.4473,-362.8713 1687.8668,-354.5678 1684.1249,-360.4838\"/>\n</g>\n<!-- 6&#45;S -->\n<g id=\"node25\" class=\"node\">\n<title>6&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1713.6798\" cy=\"-245\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"1713.6798\" y=\"-240\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 6&#45;&gt;6&#45;S -->\n<g id=\"edge46\" class=\"edge\">\n<title>6&#45;&gt;6&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1602.2323,-269.1459C1608.4854,-267.7502 1614.8839,-266.3297 1620.931,-265 1641.5073,-260.4756 1664.6741,-255.48 1682.7069,-251.6132\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1683.6274,-254.9956 1692.6728,-249.4791 1682.1615,-248.1507 1683.6274,-254.9956\"/>\n</g>\n<!-- 6&#45;W -->\n<g id=\"node26\" class=\"node\">\n<title>6&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1713.6798\" cy=\"-310\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"1713.6798\" y=\"-305\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 6&#45;&gt;6&#45;W -->\n<g id=\"edge49\" class=\"edge\">\n<title>6&#45;&gt;6&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1600.7208,-291.4456C1607.3969,-293.5142 1614.325,-295.4678 1620.931,-297 1639.688,-301.3506 1660.9385,-304.4579 1678.3618,-306.5413\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1678.3286,-310.0598 1688.6591,-307.7081 1679.1168,-303.1044 1678.3286,-310.0598\"/>\n</g>\n<!-- 3&#45;W&#45;&gt;2 -->\n<g id=\"edge20\" class=\"edge\">\n<title>3&#45;W&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2071.0675,-148C2098.292,-148 2142.6589,-148 2177.2727,-148\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2177.672,-151.5001 2187.672,-148 2177.6719,-144.5001 2177.672,-151.5001\"/>\n<text text-anchor=\"middle\" x=\"2129.426\" y=\"-153.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 3&#45;W&#45;&gt;3 -->\n<g id=\"edge21\" class=\"edge\">\n<title>3&#45;W&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2021.8302,-154.0817C2002.2004,-159.7463 1974.4924,-169.5153 1953.4285,-184 1943.6811,-190.7029 1934.4332,-199.3524 1926.3028,-208.1122\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1923.4993,-205.9975 1919.461,-215.7925 1928.7261,-210.6537 1923.4993,-205.9975\"/>\n<text text-anchor=\"middle\" x=\"1978.4285\" y=\"-189.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 4&#45;N&#45;&gt;4 -->\n<g id=\"edge24\" class=\"edge\">\n<title>4&#45;N&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M463.6814,-444.0707C443.2588,-439.5362 412.2634,-432.6543 386.0746,-426.8395\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"386.8259,-423.4212 376.3049,-424.6704 385.3085,-430.2548 386.8259,-423.4212\"/>\n<text text-anchor=\"middle\" x=\"420.2548\" y=\"-444.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 7 -->\n<g id=\"node15\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"634.5097\" cy=\"-462\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"634.5097\" cy=\"-462\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"634.5097\" y=\"-455.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">7</text>\n</g>\n<!-- 4&#45;N&#45;&gt;7 -->\n<g id=\"edge23\" class=\"edge\">\n<title>4&#45;N&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M508.4556,-450.9744C528.6006,-452.7365 558.8161,-455.3793 584.6144,-457.6358\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"584.3537,-461.1263 594.6207,-458.511 584.9637,-454.1529 584.3537,-461.1263\"/>\n<text text-anchor=\"middle\" x=\"551.5097\" y=\"-461.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 7&#45;E -->\n<g id=\"node27\" class=\"node\">\n<title>7&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"781.7229\" cy=\"-478\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"781.7229\" y=\"-473\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 7&#45;&gt;7&#45;E -->\n<g id=\"edge52\" class=\"edge\">\n<title>7&#45;&gt;7&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M666.8199,-485.6646C674.8357,-490.33 683.6722,-494.5401 692.5097,-497 713.918,-502.959 720.8505,-501.9709 742.5097,-497 746.3806,-496.1116 750.313,-494.7885 754.119,-493.2464\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"755.5756,-496.4289 763.2118,-489.0846 752.6623,-490.0639 755.5756,-496.4289\"/>\n</g>\n<!-- 7&#45;S -->\n<g id=\"node28\" class=\"node\">\n<title>7&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"781.7229\" cy=\"-412\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"781.7229\" y=\"-407\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 7&#45;&gt;7&#45;S -->\n<g id=\"edge55\" class=\"edge\">\n<title>7&#45;&gt;7&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M673.5258,-452.5338C694.2924,-447.0638 720.0996,-439.5726 742.5097,-431 746.2191,-429.581 750.0563,-427.945 753.8128,-426.2405\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"755.3298,-429.3948 762.8657,-421.9476 752.3305,-423.0699 755.3298,-429.3948\"/>\n</g>\n<!-- 4&#45;E&#45;&gt;4 -->\n<g id=\"edge27\" class=\"edge\">\n<title>4&#45;E&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M465.3578,-315.221C445.9822,-310.9876 416.7706,-307.8614 395.2548,-320 376.5391,-330.5589 363.1813,-349.8538 354.0399,-368.3907\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"350.7678,-367.1285 349.7621,-377.6755 357.1255,-370.0577 350.7678,-367.1285\"/>\n<text text-anchor=\"middle\" x=\"420.2548\" y=\"-325.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 4&#45;E&#45;&gt;5 -->\n<g id=\"edge26\" class=\"edge\">\n<title>4&#45;E&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M492.0404,-300.4664C507.5493,-253.4489 552.9168,-142 634.5097,-142 634.5097,-142 634.5097,-142 1645.931,-142 1734.3601,-142 1778.0533,-130.4691 1837.4285,-196 1868.0438,-229.7893 1822.9434,-265.0042 1855.4285,-297 1894.8874,-335.8646 1923.0439,-317 1978.4285,-317 1978.4285,-317 1978.4285,-317 2378.6747,-317 2423.4313,-317 2474.0531,-322.7699 2510.4842,-327.9505\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2510.4381,-331.4807 2520.8386,-329.4613 2511.4488,-324.5541 2510.4381,-331.4807\"/>\n<text text-anchor=\"middle\" x=\"1464.431\" y=\"-165.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8 &#160;</text>\n<text text-anchor=\"middle\" x=\"1464.431\" y=\"-147.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 4&#45;S&#45;&gt;1 -->\n<g id=\"edge29\" class=\"edge\">\n<title>4&#45;S&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M467.3986,-593.4188C448.5163,-602.5894 418.721,-612.9195 395.2548,-602 225.0565,-522.8013 107.2161,-317.4725 61.3387,-225.6015\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"64.4335,-223.9637 56.8674,-216.5472 58.1571,-227.0633 64.4335,-223.9637\"/>\n<text text-anchor=\"middle\" x=\"254.2548\" y=\"-527.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 4&#45;S&#45;&gt;4 -->\n<g id=\"edge30\" class=\"edge\">\n<title>4&#45;S&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M464.3221,-583.1447C444.1967,-582.1527 414.4621,-577.6506 395.2548,-561 367.341,-536.8018 352.7107,-497.2864 345.1313,-465.6136\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"348.4994,-464.6359 342.9103,-455.6352 341.6666,-466.1567 348.4994,-464.6359\"/>\n<text text-anchor=\"middle\" x=\"420.2548\" y=\"-585.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 5&#45;N&#45;&gt;5 -->\n<g id=\"edge33\" class=\"edge\">\n<title>5&#45;N&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2731.7108,-383.8689C2724.3671,-371.9121 2713.3736,-357.6351 2699.4234,-350 2672.5398,-335.2863 2638.3309,-331.6867 2610.5776,-331.8144\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2610.21,-328.3213 2600.2857,-332.0304 2610.357,-335.3197 2610.21,-328.3213\"/>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-373.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2 &#160;</text>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-355.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 8 -->\n<g id=\"node19\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"928.9361\" cy=\"-416\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"928.9361\" cy=\"-416\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"928.9361\" y=\"-409.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">8</text>\n</g>\n<!-- 5&#45;N&#45;&gt;8 -->\n<g id=\"edge32\" class=\"edge\">\n<title>5&#45;N&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2722.4598,-415.3307C2689.8578,-432.9714 2622.1641,-465 2560.4234,-465 1313.4335,-465 1313.4335,-465 1313.4335,-465 1198.1305,-465 1169.6515,-454.6265 1054.9361,-443 1024.6605,-439.9316 1016.6541,-441.547 986.9361,-435 983.5213,-434.2477 980.0231,-433.3737 976.5198,-432.4217\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"977.2773,-428.9972 966.6979,-429.5692 975.3249,-435.7194 977.2773,-428.9972\"/>\n<text text-anchor=\"middle\" x=\"1796.9285\" y=\"-470.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 8&#45;E -->\n<g id=\"node29\" class=\"node\">\n<title>8&#45;E</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1079.6848\" cy=\"-325\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"1079.6848\" y=\"-320\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">E</text>\n</g>\n<!-- 8&#45;&gt;8&#45;E -->\n<g id=\"edge58\" class=\"edge\">\n<title>8&#45;&gt;8&#45;E</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M963.5209,-395.1228C990.6174,-378.7659 1027.7781,-356.3336 1052.7046,-341.2867\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1054.5408,-344.2666 1061.2931,-336.1022 1050.9232,-338.2738 1054.5408,-344.2666\"/>\n</g>\n<!-- 8&#45;S -->\n<g id=\"node30\" class=\"node\">\n<title>8&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1079.6848\" cy=\"-413\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"1079.6848\" y=\"-408\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 8&#45;&gt;8&#45;S -->\n<g id=\"edge61\" class=\"edge\">\n<title>8&#45;&gt;8&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M966.573,-401.7725C973.2661,-399.7909 980.2394,-398.0689 986.9361,-397 1008.8805,-393.4972 1015.0692,-393.0419 1036.9361,-397 1041.4441,-397.816 1046.0963,-399.0794 1050.5974,-400.5466\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1049.5014,-403.8721 1060.0956,-403.9893 1051.8868,-397.2911 1049.5014,-403.8721\"/>\n</g>\n<!-- 8&#45;W -->\n<g id=\"node31\" class=\"node\">\n<title>8&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1079.6848\" cy=\"-515\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"1079.6848\" y=\"-510\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 8&#45;&gt;8&#45;W -->\n<g id=\"edge64\" class=\"edge\">\n<title>8&#45;&gt;8&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M945.2814,-452.8235C954.9042,-470.2063 968.7829,-489.5958 986.9361,-501 1003.96,-511.6948 1026.1346,-515.3303 1044.5329,-516.2281\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1044.5022,-519.7283 1054.5845,-516.4729 1044.6728,-512.7303 1044.5022,-519.7283\"/>\n</g>\n<!-- 5&#45;E&#45;&gt;5 -->\n<g id=\"edge36\" class=\"edge\">\n<title>5&#45;E&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2729.8435,-42.5613C2706.7175,-75.8301 2655.7942,-150.6603 2618.4234,-217 2605.0987,-240.6538 2591.7217,-267.8154 2581.1778,-290.2114\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2577.9029,-288.9523 2576.8408,-299.4938 2584.2448,-291.9154 2577.9029,-288.9523\"/>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-240.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2 &#160;</text>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-222.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 5&#45;E&#45;&gt;6 -->\n<g id=\"edge35\" class=\"edge\">\n<title>5&#45;E&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2721.4374,-18.991C2714.4888,-17.1581 2706.6665,-15.289 2699.4234,-14 2638.2936,-3.1213 2622.5138,0 2560.4234,0 2310.926,0 2310.926,0 2310.926,0 2057.6985,0 1990.8227,-35.1694 1756.4285,-131 1691.2734,-157.6382 1675.0741,-168.0188 1620.931,-213 1611.6608,-220.7016 1602.5419,-229.9066 1594.3784,-238.9329\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1591.7513,-236.6203 1587.7634,-246.436 1597.002,-241.2496 1591.7513,-236.6203\"/>\n<text text-anchor=\"middle\" x=\"2129.426\" y=\"-21.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 5&#45;S&#45;&gt;2 -->\n<g id=\"edge38\" class=\"edge\">\n<title>5&#45;S&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2725.1004,-255.8496C2677.5026,-220.1219 2537.9624,-121.6896 2403.4234,-88 2352.4573,-75.2377 2333.5737,-78.8612 2285.926,-101 2278.6534,-104.3791 2271.508,-108.8704 2264.8533,-113.7389\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2262.4705,-111.1577 2256.7138,-120.0521 2266.7607,-116.6889 2262.4705,-111.1577\"/>\n<text text-anchor=\"middle\" x=\"2461.9234\" y=\"-128.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 5&#45;S&#45;&gt;5 -->\n<g id=\"edge39\" class=\"edge\">\n<title>5&#45;S&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2721.75,-263.1637C2696.2383,-257.0344 2651.8752,-250.2746 2618.4234,-266 2605.9036,-271.8855 2595.1574,-281.9475 2586.4382,-292.6359\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2583.4406,-290.7953 2580.1698,-300.8726 2589.011,-295.0345 2583.4406,-290.7953\"/>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-289.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2 &#160;</text>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-271.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 5&#45;W&#45;&gt;4 -->\n<g id=\"edge41\" class=\"edge\">\n<title>5&#45;W&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2722.1665,-537.9122C2715.2666,-542.422 2707.2887,-546.9751 2699.4234,-550 2640.9098,-572.5038 2623.1153,-574 2560.4234,-574 845.9361,-574 845.9361,-574 845.9361,-574 670.9051,-574 632.3024,-526.3735 463.2548,-481 432.8294,-472.8336 423.2991,-475.3498 395.2548,-461 388.6247,-457.6075 381.9992,-453.4189 375.7218,-448.9745\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"377.5258,-445.9548 367.4096,-442.8062 373.3543,-451.5761 377.5258,-445.9548\"/>\n<text text-anchor=\"middle\" x=\"1464.431\" y=\"-579.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 5&#45;W&#45;&gt;5 -->\n<g id=\"edge42\" class=\"edge\">\n<title>5&#45;W&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M2717.806,-516.6271C2690.4619,-508.3619 2646.2913,-491.5178 2618.4234,-463 2597.1675,-441.2483 2582.8149,-410.3109 2573.7047,-384.4171\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2576.9667,-383.136 2570.4663,-374.7696 2570.3305,-385.3636 2576.9667,-383.136\"/>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-533.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2 &#160;</text>\n<text text-anchor=\"middle\" x=\"2658.9234\" y=\"-515.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 6&#45;N&#45;&gt;6 -->\n<g id=\"edge45\" class=\"edge\">\n<title>6&#45;N&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1691.1782,-371.6039C1671.3908,-367.8079 1642.4681,-360.2283 1620.931,-346 1610.1025,-338.8462 1600.1585,-329.1083 1591.6769,-319.2384\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1594.2476,-316.8535 1585.1994,-311.3415 1588.8353,-321.2929 1594.2476,-316.8535\"/>\n<text text-anchor=\"middle\" x=\"1645.931\" y=\"-371.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 9 -->\n<g id=\"node24\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"#85ff75\" stroke=\"#85ff75\" cx=\"1230.4335\" cy=\"-324\" rx=\"36\" ry=\"36\"/>\n<ellipse fill=\"none\" stroke=\"#85ff75\" cx=\"1230.4335\" cy=\"-324\" rx=\"40\" ry=\"40\"/>\n<text text-anchor=\"middle\" x=\"1230.4335\" y=\"-317.8\" font-family=\"Arial\" font-size=\"24.00\" fill=\"#000000\">9</text>\n</g>\n<!-- 6&#45;N&#45;&gt;9 -->\n<g id=\"edge44\" class=\"edge\">\n<title>6&#45;N&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1692.217,-382.8776C1685.4965,-384.9762 1677.9901,-386.9504 1670.931,-388 1648.9505,-391.2683 1643.1308,-388.9995 1620.931,-388 1472.7042,-381.3266 1429.7334,-403.2777 1288.4335,-358 1283.1538,-356.3082 1277.8365,-354.0736 1272.6774,-351.556\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1274.0826,-348.3403 1263.6041,-346.7747 1270.8192,-354.5331 1274.0826,-348.3403\"/>\n<text text-anchor=\"middle\" x=\"1464.431\" y=\"-392.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 9&#45;S -->\n<g id=\"node32\" class=\"node\">\n<title>9&#45;S</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1381.1823\" cy=\"-326\" rx=\"21.4286\" ry=\"21.4286\"/>\n<text text-anchor=\"middle\" x=\"1381.1823\" y=\"-321\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">S</text>\n</g>\n<!-- 9&#45;&gt;9&#45;S -->\n<g id=\"edge67\" class=\"edge\">\n<title>9&#45;&gt;9&#45;S</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1270.4357,-324.5307C1295.1344,-324.8584 1326.3866,-325.273 1349.3893,-325.5782\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1349.5932,-329.0811 1359.6388,-325.7142 1349.6861,-322.0817 1349.5932,-329.0811\"/>\n</g>\n<!-- 9&#45;W -->\n<g id=\"node33\" class=\"node\">\n<title>9&#45;W</title>\n<ellipse fill=\"#ffb6c1\" stroke=\"#ffb6c1\" cx=\"1381.1823\" cy=\"-262\" rx=\"25\" ry=\"25\"/>\n<text text-anchor=\"middle\" x=\"1381.1823\" y=\"-257\" font-family=\"Arial\" font-size=\"20.00\" fill=\"#000000\">W</text>\n</g>\n<!-- 9&#45;&gt;9&#45;W -->\n<g id=\"edge70\" class=\"edge\">\n<title>9&#45;&gt;9&#45;W</title>\n<path fill=\"none\" stroke=\"#ff0000\" stroke-width=\"2\" d=\"M1268.3642,-311.2132C1289.2764,-303.8499 1315.5617,-294.077 1338.4335,-284 1342.3004,-282.2963 1346.3097,-280.4061 1350.2538,-278.4698\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" stroke-width=\"2\" points=\"1351.8904,-281.5643 1359.2408,-273.934 1348.7364,-275.3151 1351.8904,-281.5643\"/>\n</g>\n<!-- 6&#45;S&#45;&gt;3 -->\n<g id=\"edge47\" class=\"edge\">\n<title>6&#45;S&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1735.2424,-245.3559C1762.1152,-245.7995 1808.8968,-246.5717 1844.9892,-247.1674\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1845.2791,-250.6726 1855.3356,-247.3382 1845.3947,-243.6736 1845.2791,-250.6726\"/>\n<text text-anchor=\"middle\" x=\"1796.9285\" y=\"-253.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 6&#45;S&#45;&gt;6 -->\n<g id=\"edge48\" class=\"edge\">\n<title>6&#45;S&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1693.6157,-237.299C1674.4676,-231.126 1645.0474,-224.7416 1620.931,-233 1613.5553,-235.5257 1606.3928,-239.4139 1599.7627,-243.869\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1597.6997,-241.0416 1591.6734,-249.7557 1601.8185,-246.7016 1597.6997,-241.0416\"/>\n<text text-anchor=\"middle\" x=\"1645.931\" y=\"-238.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 6&#45;W&#45;&gt;5 -->\n<g id=\"edge50\" class=\"edge\">\n<title>6&#45;W&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1736.7533,-319.5433C1770.8335,-332.7527 1836.917,-355 1895.4285,-355 1895.4285,-355 1895.4285,-355 2378.6747,-355 2423.4313,-355 2474.0531,-349.2301 2510.4842,-344.0495\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2511.4488,-347.4459 2520.8386,-342.5387 2510.4381,-340.5193 2511.4488,-347.4459\"/>\n<text text-anchor=\"middle\" x=\"2129.426\" y=\"-378.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8 &#160;</text>\n<text text-anchor=\"middle\" x=\"2129.426\" y=\"-360.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 6&#45;W&#45;&gt;6 -->\n<g id=\"edge51\" class=\"edge\">\n<title>6&#45;W&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1696.711,-291.5549C1689.4809,-285.0082 1680.506,-278.4197 1670.931,-275 1652.5241,-268.426 1631.2081,-267.7008 1612.401,-269.1523\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1611.851,-265.6896 1602.2452,-270.1592 1612.5418,-272.6554 1611.851,-265.6896\"/>\n<text text-anchor=\"middle\" x=\"1645.931\" y=\"-280.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 7&#45;E&#45;&gt;7 -->\n<g id=\"edge54\" class=\"edge\">\n<title>7&#45;E&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M760.4973,-475.6931C740.6786,-473.5391 710.3455,-470.2423 684.4091,-467.4234\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"684.6673,-463.9309 674.3477,-466.3298 683.9109,-470.8899 684.6673,-463.9309\"/>\n<text text-anchor=\"middle\" x=\"717.5097\" y=\"-478.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 7&#45;E&#45;&gt;8 -->\n<g id=\"edge53\" class=\"edge\">\n<title>7&#45;E&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M801.5767,-469.6384C822.2169,-460.9456 855.2561,-447.0309 882.5108,-435.5524\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"884.0225,-438.7135 891.88,-431.6064 881.3055,-432.2623 884.0225,-438.7135\"/>\n<text text-anchor=\"middle\" x=\"845.9361\" y=\"-466.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 7&#45;S&#45;&gt;4 -->\n<g id=\"edge56\" class=\"edge\">\n<title>7&#45;S&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M761.68,-403.9882C755.6206,-401.9409 748.8821,-400.0288 742.5097,-399 720.5715,-395.4583 714.7255,-398.4688 692.5097,-399 583.5933,-401.6045 456.0548,-408.6944 387.3031,-412.8496\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"386.959,-409.3639 377.1902,-413.4653 387.3845,-416.351 386.959,-409.3639\"/>\n<text text-anchor=\"middle\" x=\"551.5097\" y=\"-410.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 7&#45;S&#45;&gt;7 -->\n<g id=\"edge57\" class=\"edge\">\n<title>7&#45;S&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M761.1166,-406.8385C742.5235,-403.2051 714.7586,-400.3656 692.5097,-409 683.638,-412.4429 675.2943,-417.9455 667.8496,-424.1295\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"665.2497,-421.7597 660.1417,-431.0419 669.9232,-426.9711 665.2497,-421.7597\"/>\n<text text-anchor=\"middle\" x=\"717.5097\" y=\"-414.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 8&#45;E&#45;&gt;8 -->\n<g id=\"edge60\" class=\"edge\">\n<title>8&#45;E&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1058.6825,-321.064C1038.8553,-318.4639 1008.9613,-317.5161 986.9361,-330 970.9581,-339.0564 958.5058,-354.5456 949.3058,-369.905\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"946.1083,-368.4508 944.2628,-378.8837 952.2115,-371.8788 946.1083,-368.4508\"/>\n<text text-anchor=\"middle\" x=\"1011.9361\" y=\"-335.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 8&#45;E&#45;&gt;9 -->\n<g id=\"edge59\" class=\"edge\">\n<title>8&#45;E&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1100.9637,-324.8588C1121.4741,-324.7228 1153.2793,-324.5118 1180.2327,-324.333\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1180.3151,-327.8326 1190.2916,-324.2663 1180.2686,-320.8328 1180.3151,-327.8326\"/>\n<text text-anchor=\"middle\" x=\"1147.4335\" y=\"-330.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 8&#45;S&#45;&gt;5 -->\n<g id=\"edge62\" class=\"edge\">\n<title>8&#45;S&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1101.0957,-416.2471C1130.2685,-420.3828 1184.1732,-427 1230.4335,-427 1230.4335,-427 1230.4335,-427 1645.931,-427 1695.839,-427 1706.7368,-411.6411 1756.4285,-407 1817.9402,-401.2549 1833.6491,-406 1895.4285,-406 1895.4285,-406 1895.4285,-406 2378.6747,-406 2429.0165,-406 2482.2758,-382.3241 2518.0322,-362.5108\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2519.7632,-365.5528 2526.7333,-357.5735 2516.3086,-359.4646 2519.7632,-365.5528\"/>\n<text text-anchor=\"middle\" x=\"1796.9285\" y=\"-430.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8 &#160;</text>\n<text text-anchor=\"middle\" x=\"1796.9285\" y=\"-412.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">reward =10</text>\n</g>\n<!-- 8&#45;S&#45;&gt;8 -->\n<g id=\"edge63\" class=\"edge\">\n<title>8&#45;S&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1058.1413,-413.4287C1037.5792,-413.8379 1005.8404,-414.4696 978.9663,-415.0044\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"978.8665,-411.5056 968.9382,-415.2039 979.0059,-418.5042 978.8665,-411.5056\"/>\n<text text-anchor=\"middle\" x=\"1011.9361\" y=\"-419.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 8&#45;W&#45;&gt;7 -->\n<g id=\"edge65\" class=\"edge\">\n<title>8&#45;W&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1055.4297,-520.7174C992.6915,-534.2963 820.9988,-563.7122 692.5097,-513 684.1727,-509.7096 676.2352,-504.6604 669.0568,-498.9953\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"670.8819,-495.9521 661.0004,-492.1304 666.3419,-501.2802 670.8819,-495.9521\"/>\n<text text-anchor=\"middle\" x=\"845.9361\" y=\"-546.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 8&#45;W&#45;&gt;8 -->\n<g id=\"edge66\" class=\"edge\">\n<title>8&#45;W&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1061.5692,-497.4061C1054.321,-491.0149 1045.6347,-484.1146 1036.9361,-479 1016.4433,-466.9509 1007.7289,-471.5236 986.9361,-460 980.5519,-456.4618 974.0965,-452.2808 967.927,-447.9271\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"969.8622,-445.0063 959.7246,-441.9274 965.7295,-450.6562 969.8622,-445.0063\"/>\n<text text-anchor=\"middle\" x=\"1011.9361\" y=\"-484.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 9&#45;S&#45;&gt;6 -->\n<g id=\"edge68\" class=\"edge\">\n<title>9&#45;S&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1402.0312,-320.4938C1429.1338,-313.336 1477.3218,-300.6095 1513.9188,-290.9442\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1515.1317,-294.2439 1523.9065,-288.3064 1513.3443,-287.476 1515.1317,-294.2439\"/>\n<text text-anchor=\"middle\" x=\"1464.431\" y=\"-320.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 9&#45;S&#45;&gt;9 -->\n<g id=\"edge69\" class=\"edge\">\n<title>9&#45;S&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1360.4175,-330.6841C1353.467,-332.0381 1345.651,-333.3305 1338.4335,-334 1316.3063,-336.0526 1310.5748,-335.8948 1288.4335,-334 1285.7118,-333.7671 1282.9259,-333.4738 1280.1191,-333.1352\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1280.3234,-329.6305 1269.9403,-331.7382 1279.3715,-336.5655 1280.3234,-329.6305\"/>\n<text text-anchor=\"middle\" x=\"1313.4335\" y=\"-341.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n<!-- 9&#45;W&#45;&gt;8 -->\n<g id=\"edge71\" class=\"edge\">\n<title>9&#45;W&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1357.422,-254.7566C1351.2943,-253.2015 1344.6678,-251.7892 1338.4335,-251 1316.3873,-248.209 1310.5595,-248.9335 1288.4335,-251 1141.0281,-264.7673 1031.7917,-280.2894 986.9361,-316 969.6643,-329.7505 956.5268,-350.1473 947.1597,-368.9399\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"943.9854,-367.4652 942.8625,-378.0004 950.3101,-370.4649 943.9854,-367.4652\"/>\n<text text-anchor=\"middle\" x=\"1147.4335\" y=\"-276.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.8</text>\n</g>\n<!-- 9&#45;W&#45;&gt;9 -->\n<g id=\"edge72\" class=\"edge\">\n<title>9&#45;W&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#0000ff\" stroke-dasharray=\"5,2\" d=\"M1356.8675,-256.8761C1337.5066,-253.9772 1310.1481,-252.5787 1288.4335,-262 1277.6616,-266.6736 1267.9772,-274.4425 1259.7467,-282.9275\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1257.0042,-280.7422 1252.8973,-290.5087 1262.1983,-285.4349 1257.0042,-280.7422\"/>\n<text text-anchor=\"middle\" x=\"1313.4335\" y=\"-267.2\" font-family=\"Arial\" font-size=\"16.00\" fill=\"#000000\">p = 0.2</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f665ca37b20>"
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the **Value Iteration** algoritm"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# parameters\r\n",
        "discount = 0.9  # discount, as specified by the question\r\n",
        "num_iter = 1000 # maximum iterations, excluding initialization\r\n",
        "threshold = 0.001 # stop VI algorithm if new values are this close to old values (or closer)\r\n",
        "\r\n",
        "# initialize output env\r\n",
        "rows, cols = 3,3\r\n",
        "env = np.zeros((rows, cols))\r\n",
        "env[1][1] = 10\r\n",
        "print('Initial state:\\n\\n', env, end='\\n\\n')\r\n",
        "\r\n",
        "# initialize V(s) -> V_0\r\n",
        "state_values = {s : 0 for s in mdp.get_states()}\r\n",
        "\r\n",
        "for i in range(num_iter):\r\n",
        "\r\n",
        "  # Compute new state values using the functions defined above.\r\n",
        "  new_state_values = { s: get_new_state_value(mdp, state_values, s, discount)\r\n",
        "    for s in mdp.get_states()\r\n",
        "  }\r\n",
        "\r\n",
        "  # print iteration result\r\n",
        "  print(\"iter %i:\\n\"%(i+1))\r\n",
        "  render(env, new_state_values)\r\n",
        "\r\n",
        "  # Compute difference\r\n",
        "  diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_states())\r\n",
        "\r\n",
        "  # update state values for next iteration\r\n",
        "  state_values = new_state_values\r\n",
        "\r\n",
        "  # stop VI algorithm once diff is below set threshold\r\n",
        "  if diff < threshold:\r\n",
        "      print(\"Terminated, final state values after %i itertions:\" %i); break\r\n",
        "render(env, new_state_values)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state:\n",
            "\n",
            " [[ 0.  0.  0.]\n",
            " [ 0. 10.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "\n",
            "iter 1:\n",
            "\n",
            "[[0. 8. 0.]\n",
            " [8. 2. 8.]\n",
            " [0. 8. 0.]]\n",
            "\n",
            "iter 2:\n",
            "\n",
            "[[ 5.76 10.88  5.76]\n",
            " [10.88  8.12 10.88]\n",
            " [ 5.76 10.88  5.76]]\n",
            "\n",
            "iter 3:\n",
            "\n",
            "[[ 8.8704 15.8048  8.8704]\n",
            " [15.8048 11.2952 15.8048]\n",
            " [ 8.8704 15.8048  8.8704]]\n",
            "\n",
            "iter 4:\n",
            "\n",
            "[[12.9761 18.9774 12.9761]\n",
            " [18.9774 15.4126 18.9774]\n",
            " [12.9761 18.9774 12.9761]]\n",
            "\n",
            "iter 5:\n",
            "\n",
            "[[15.9994 22.513  15.9994]\n",
            " [22.513  18.438  22.513 ]\n",
            " [15.9994 22.513  15.9994]]\n",
            "\n",
            "iter 6:\n",
            "\n",
            "[[19.0893 25.3277 19.0893]\n",
            " [25.3277 21.5282 25.3277]\n",
            " [19.0893 25.3277 19.0893]]\n",
            "\n",
            "iter 7:\n",
            "\n",
            "[[21.672  28.0593 21.672 ]\n",
            " [28.0593 24.111  28.0593]\n",
            " [21.672  28.0593 21.672 ]]\n",
            "\n",
            "iter 8:\n",
            "\n",
            "[[24.1037 30.4106 24.1037]\n",
            " [30.4106 26.5427 30.4106]\n",
            " [24.1037 30.4106 24.1037]]\n",
            "\n",
            "iter 9:\n",
            "\n",
            "[[26.2343 32.5846 26.2343]\n",
            " [32.5846 28.6733 32.5846]\n",
            " [26.2343 32.5846 26.2343]]\n",
            "\n",
            "iter 10:\n",
            "\n",
            "[[28.1831 34.51   28.1831]\n",
            " [34.51   30.6221 34.51  ]\n",
            " [28.1831 34.51   28.1831]]\n",
            "\n",
            "iter 11:\n",
            "\n",
            "[[29.9202 36.2597 29.9202]\n",
            " [36.2597 32.3592 36.2597]\n",
            " [29.9202 36.2597 29.9202]]\n",
            "\n",
            "iter 12:\n",
            "\n",
            "[[31.4926 37.8254 31.4926]\n",
            " [37.8254 33.9317 37.8254]\n",
            " [31.4926 37.8254 31.4926]]\n",
            "\n",
            "iter 13:\n",
            "\n",
            "[[32.9029 39.2394 32.9029]\n",
            " [39.2394 35.342  39.2394]\n",
            " [32.9029 39.2394 32.9029]]\n",
            "\n",
            "iter 14:\n",
            "\n",
            "[[34.1749 40.5093 34.1749]\n",
            " [40.5093 36.6139 40.5093]\n",
            " [34.1749 40.5093 34.1749]]\n",
            "\n",
            "iter 15:\n",
            "\n",
            "[[35.3182 41.6537 35.3182]\n",
            " [41.6537 37.7572 41.6537]\n",
            " [35.3182 41.6537 35.3182]]\n",
            "\n",
            "iter 16:\n",
            "\n",
            "[[36.3479 42.6828 36.3479]\n",
            " [42.6828 38.7869 42.6828]\n",
            " [36.3479 42.6828 36.3479]]\n",
            "\n",
            "iter 17:\n",
            "\n",
            "[[37.2743 43.6095 37.2743]\n",
            " [43.6095 39.7133 43.6095]\n",
            " [37.2743 43.6095 37.2743]]\n",
            "\n",
            "iter 18:\n",
            "\n",
            "[[38.1082 44.4433 38.1082]\n",
            " [44.4433 40.5472 44.4433]\n",
            " [38.1082 44.4433 38.1082]]\n",
            "\n",
            "iter 19:\n",
            "\n",
            "[[38.8586 45.1938 38.8586]\n",
            " [45.1938 41.2977 45.1938]\n",
            " [38.8586 45.1938 38.8586]]\n",
            "\n",
            "iter 20:\n",
            "\n",
            "[[39.5341 45.8692 39.5341]\n",
            " [45.8692 41.9731 45.8692]\n",
            " [39.5341 45.8692 39.5341]]\n",
            "\n",
            "iter 21:\n",
            "\n",
            "[[40.142  46.4771 40.142 ]\n",
            " [46.4771 42.581  46.4771]\n",
            " [40.142  46.4771 40.142 ]]\n",
            "\n",
            "iter 22:\n",
            "\n",
            "[[40.6891 47.0242 40.6891]\n",
            " [47.0242 43.1281 47.0242]\n",
            " [40.6891 47.0242 40.6891]]\n",
            "\n",
            "iter 23:\n",
            "\n",
            "[[41.1815 47.5166 41.1815]\n",
            " [47.5166 43.6205 47.5166]\n",
            " [41.1815 47.5166 41.1815]]\n",
            "\n",
            "iter 24:\n",
            "\n",
            "[[41.6246 47.9597 41.6246]\n",
            " [47.9597 44.0636 47.9597]\n",
            " [41.6246 47.9597 41.6246]]\n",
            "\n",
            "iter 25:\n",
            "\n",
            "[[42.0234 48.3586 42.0234]\n",
            " [48.3586 44.4625 48.3586]\n",
            " [42.0234 48.3586 42.0234]]\n",
            "\n",
            "iter 26:\n",
            "\n",
            "[[42.3824 48.7175 42.3824]\n",
            " [48.7175 44.8214 48.7175]\n",
            " [42.3824 48.7175 42.3824]]\n",
            "\n",
            "iter 27:\n",
            "\n",
            "[[42.7054 49.0406 42.7054]\n",
            " [49.0406 45.1445 49.0406]\n",
            " [42.7054 49.0406 42.7054]]\n",
            "\n",
            "iter 28:\n",
            "\n",
            "[[42.9962 49.3313 42.9962]\n",
            " [49.3313 45.4352 49.3313]\n",
            " [42.9962 49.3313 42.9962]]\n",
            "\n",
            "iter 29:\n",
            "\n",
            "[[43.2579 49.593  43.2579]\n",
            " [49.593  45.6969 49.593 ]\n",
            " [43.2579 49.593  43.2579]]\n",
            "\n",
            "iter 30:\n",
            "\n",
            "[[43.4934 49.8285 43.4934]\n",
            " [49.8285 45.9324 49.8285]\n",
            " [43.4934 49.8285 43.4934]]\n",
            "\n",
            "iter 31:\n",
            "\n",
            "[[43.7053 50.0404 43.7053]\n",
            " [50.0404 46.1443 50.0404]\n",
            " [43.7053 50.0404 43.7053]]\n",
            "\n",
            "iter 32:\n",
            "\n",
            "[[43.8961 50.2312 43.8961]\n",
            " [50.2312 46.3351 50.2312]\n",
            " [43.8961 50.2312 43.8961]]\n",
            "\n",
            "iter 33:\n",
            "\n",
            "[[44.0678 50.4029 44.0678]\n",
            " [50.4029 46.5068 50.4029]\n",
            " [44.0678 50.4029 44.0678]]\n",
            "\n",
            "iter 34:\n",
            "\n",
            "[[44.2223 50.5574 44.2223]\n",
            " [50.5574 46.6613 50.5574]\n",
            " [44.2223 50.5574 44.2223]]\n",
            "\n",
            "iter 35:\n",
            "\n",
            "[[44.3613 50.6965 44.3613]\n",
            " [50.6965 46.8004 50.6965]\n",
            " [44.3613 50.6965 44.3613]]\n",
            "\n",
            "iter 36:\n",
            "\n",
            "[[44.4865 50.8216 44.4865]\n",
            " [50.8216 46.9255 50.8216]\n",
            " [44.4865 50.8216 44.4865]]\n",
            "\n",
            "iter 37:\n",
            "\n",
            "[[44.5991 50.9343 44.5991]\n",
            " [50.9343 47.0382 50.9343]\n",
            " [44.5991 50.9343 44.5991]]\n",
            "\n",
            "iter 38:\n",
            "\n",
            "[[44.7005 51.0357 44.7005]\n",
            " [51.0357 47.1395 51.0357]\n",
            " [44.7005 51.0357 44.7005]]\n",
            "\n",
            "iter 39:\n",
            "\n",
            "[[44.7918 51.1269 44.7918]\n",
            " [51.1269 47.2308 51.1269]\n",
            " [44.7918 51.1269 44.7918]]\n",
            "\n",
            "iter 40:\n",
            "\n",
            "[[44.8739 51.209  44.8739]\n",
            " [51.209  47.3129 51.209 ]\n",
            " [44.8739 51.209  44.8739]]\n",
            "\n",
            "iter 41:\n",
            "\n",
            "[[44.9478 51.2829 44.9478]\n",
            " [51.2829 47.3868 51.2829]\n",
            " [44.9478 51.2829 44.9478]]\n",
            "\n",
            "iter 42:\n",
            "\n",
            "[[45.0143 51.3494 45.0143]\n",
            " [51.3494 47.4533 51.3494]\n",
            " [45.0143 51.3494 45.0143]]\n",
            "\n",
            "iter 43:\n",
            "\n",
            "[[45.0742 51.4093 45.0742]\n",
            " [51.4093 47.5132 51.4093]\n",
            " [45.0742 51.4093 45.0742]]\n",
            "\n",
            "iter 44:\n",
            "\n",
            "[[45.128  51.4632 45.128 ]\n",
            " [51.4632 47.5671 51.4632]\n",
            " [45.128  51.4632 45.128 ]]\n",
            "\n",
            "iter 45:\n",
            "\n",
            "[[45.1765 51.5117 45.1765]\n",
            " [51.5117 47.6155 51.5117]\n",
            " [45.1765 51.5117 45.1765]]\n",
            "\n",
            "iter 46:\n",
            "\n",
            "[[45.2202 51.5553 45.2202]\n",
            " [51.5553 47.6592 51.5553]\n",
            " [45.2202 51.5553 45.2202]]\n",
            "\n",
            "iter 47:\n",
            "\n",
            "[[45.2594 51.5946 45.2594]\n",
            " [51.5946 47.6985 51.5946]\n",
            " [45.2594 51.5946 45.2594]]\n",
            "\n",
            "iter 48:\n",
            "\n",
            "[[45.2948 51.6299 45.2948]\n",
            " [51.6299 47.7338 51.6299]\n",
            " [45.2948 51.6299 45.2948]]\n",
            "\n",
            "iter 49:\n",
            "\n",
            "[[45.3266 51.6617 45.3266]\n",
            " [51.6617 47.7656 51.6617]\n",
            " [45.3266 51.6617 45.3266]]\n",
            "\n",
            "iter 50:\n",
            "\n",
            "[[45.3552 51.6904 45.3552]\n",
            " [51.6904 47.7943 51.6904]\n",
            " [45.3552 51.6904 45.3552]]\n",
            "\n",
            "iter 51:\n",
            "\n",
            "[[45.381  51.7161 45.381 ]\n",
            " [51.7161 47.82   51.7161]\n",
            " [45.381  51.7161 45.381 ]]\n",
            "\n",
            "iter 52:\n",
            "\n",
            "[[45.4042 51.7393 45.4042]\n",
            " [51.7393 47.8432 51.7393]\n",
            " [45.4042 51.7393 45.4042]]\n",
            "\n",
            "iter 53:\n",
            "\n",
            "[[45.4251 51.7602 45.4251]\n",
            " [51.7602 47.8641 51.7602]\n",
            " [45.4251 51.7602 45.4251]]\n",
            "\n",
            "iter 54:\n",
            "\n",
            "[[45.4439 51.779  45.4439]\n",
            " [51.779  47.8829 51.779 ]\n",
            " [45.4439 51.779  45.4439]]\n",
            "\n",
            "iter 55:\n",
            "\n",
            "[[45.4608 51.7959 45.4608]\n",
            " [51.7959 47.8998 51.7959]\n",
            " [45.4608 51.7959 45.4608]]\n",
            "\n",
            "iter 56:\n",
            "\n",
            "[[45.476  51.8111 45.476 ]\n",
            " [51.8111 47.915  51.8111]\n",
            " [45.476  51.8111 45.476 ]]\n",
            "\n",
            "iter 57:\n",
            "\n",
            "[[45.4897 51.8248 45.4897]\n",
            " [51.8248 47.9287 51.8248]\n",
            " [45.4897 51.8248 45.4897]]\n",
            "\n",
            "iter 58:\n",
            "\n",
            "[[45.502  51.8371 45.502 ]\n",
            " [51.8371 47.941  51.8371]\n",
            " [45.502  51.8371 45.502 ]]\n",
            "\n",
            "iter 59:\n",
            "\n",
            "[[45.5131 51.8482 45.5131]\n",
            " [51.8482 47.9521 51.8482]\n",
            " [45.5131 51.8482 45.5131]]\n",
            "\n",
            "iter 60:\n",
            "\n",
            "[[45.5231 51.8582 45.5231]\n",
            " [51.8582 47.9621 51.8582]\n",
            " [45.5231 51.8582 45.5231]]\n",
            "\n",
            "iter 61:\n",
            "\n",
            "[[45.5321 51.8672 45.5321]\n",
            " [51.8672 47.9711 51.8672]\n",
            " [45.5321 51.8672 45.5321]]\n",
            "\n",
            "iter 62:\n",
            "\n",
            "[[45.5401 51.8753 45.5401]\n",
            " [51.8753 47.9792 51.8753]\n",
            " [45.5401 51.8753 45.5401]]\n",
            "\n",
            "iter 63:\n",
            "\n",
            "[[45.5474 51.8826 45.5474]\n",
            " [51.8826 47.9864 51.8826]\n",
            " [45.5474 51.8826 45.5474]]\n",
            "\n",
            "iter 64:\n",
            "\n",
            "[[45.554  51.8891 45.554 ]\n",
            " [51.8891 47.993  51.8891]\n",
            " [45.554  51.8891 45.554 ]]\n",
            "\n",
            "iter 65:\n",
            "\n",
            "[[45.5599 51.895  45.5599]\n",
            " [51.895  47.9989 51.895 ]\n",
            " [45.5599 51.895  45.5599]]\n",
            "\n",
            "iter 66:\n",
            "\n",
            "[[45.5652 51.9003 45.5652]\n",
            " [51.9003 48.0042 51.9003]\n",
            " [45.5652 51.9003 45.5652]]\n",
            "\n",
            "iter 67:\n",
            "\n",
            "[[45.5699 51.9051 45.5699]\n",
            " [51.9051 48.009  51.9051]\n",
            " [45.5699 51.9051 45.5699]]\n",
            "\n",
            "iter 68:\n",
            "\n",
            "[[45.5742 51.9094 45.5742]\n",
            " [51.9094 48.0133 51.9094]\n",
            " [45.5742 51.9094 45.5742]]\n",
            "\n",
            "iter 69:\n",
            "\n",
            "[[45.5781 51.9132 45.5781]\n",
            " [51.9132 48.0171 51.9132]\n",
            " [45.5781 51.9132 45.5781]]\n",
            "\n",
            "iter 70:\n",
            "\n",
            "[[45.5816 51.9167 45.5816]\n",
            " [51.9167 48.0206 51.9167]\n",
            " [45.5816 51.9167 45.5816]]\n",
            "\n",
            "iter 71:\n",
            "\n",
            "[[45.5847 51.9199 45.5847]\n",
            " [51.9199 48.0238 51.9199]\n",
            " [45.5847 51.9199 45.5847]]\n",
            "\n",
            "iter 72:\n",
            "\n",
            "[[45.5875 51.9227 45.5875]\n",
            " [51.9227 48.0266 51.9227]\n",
            " [45.5875 51.9227 45.5875]]\n",
            "\n",
            "iter 73:\n",
            "\n",
            "[[45.5901 51.9252 45.5901]\n",
            " [51.9252 48.0291 51.9252]\n",
            " [45.5901 51.9252 45.5901]]\n",
            "\n",
            "iter 74:\n",
            "\n",
            "[[45.5924 51.9275 45.5924]\n",
            " [51.9275 48.0314 51.9275]\n",
            " [45.5924 51.9275 45.5924]]\n",
            "\n",
            "iter 75:\n",
            "\n",
            "[[45.5944 51.9296 45.5944]\n",
            " [51.9296 48.0334 51.9296]\n",
            " [45.5944 51.9296 45.5944]]\n",
            "\n",
            "iter 76:\n",
            "\n",
            "[[45.5963 51.9314 45.5963]\n",
            " [51.9314 48.0353 51.9314]\n",
            " [45.5963 51.9314 45.5963]]\n",
            "\n",
            "iter 77:\n",
            "\n",
            "[[45.5979 51.9331 45.5979]\n",
            " [51.9331 48.037  51.9331]\n",
            " [45.5979 51.9331 45.5979]]\n",
            "\n",
            "iter 78:\n",
            "\n",
            "[[45.5994 51.9346 45.5994]\n",
            " [51.9346 48.0385 51.9346]\n",
            " [45.5994 51.9346 45.5994]]\n",
            "\n",
            "iter 79:\n",
            "\n",
            "[[45.6008 51.9359 45.6008]\n",
            " [51.9359 48.0398 51.9359]\n",
            " [45.6008 51.9359 45.6008]]\n",
            "\n",
            "iter 80:\n",
            "\n",
            "[[45.602  51.9371 45.602 ]\n",
            " [51.9371 48.041  51.9371]\n",
            " [45.602  51.9371 45.602 ]]\n",
            "\n",
            "iter 81:\n",
            "\n",
            "[[45.6031 51.9382 45.6031]\n",
            " [51.9382 48.0421 51.9382]\n",
            " [45.6031 51.9382 45.6031]]\n",
            "\n",
            "iter 82:\n",
            "\n",
            "[[45.6041 51.9392 45.6041]\n",
            " [51.9392 48.0431 51.9392]\n",
            " [45.6041 51.9392 45.6041]]\n",
            "\n",
            "Terminated, final state values after 81 itertions:\n",
            "[[45.6041 51.9392 45.6041]\n",
            " [51.9392 48.0431 51.9392]\n",
            " [45.6041 51.9392 45.6041]]\n",
            "\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2b)** Explain why the result of 2a) does not depend on the initial value $V_0$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason it does not depend on the initial "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning (RL)\r\n",
        "Until now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(a,s,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\r\n",
        "\r\n",
        "So far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\r\n",
        "\r\n",
        "$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s,s') + \\gamma V^\\pi(s')]$$\r\n",
        "\r\n",
        "The value function and the action-value function are directly related through\r\n",
        "\r\n",
        "$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\r\n",
        "\r\n",
        "i.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\r\n",
        "\r\n",
        "$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s\r\n",
        "]\\,s') + \\gamma V^*(s')]$$\r\n",
        "\r\n",
        "and the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\r\n",
        "\r\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\r\n",
        "\r\n",
        "## Q-learning\r\n",
        "\r\n",
        "Q-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\r\n",
        "\r\n",
        "$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \r\n",
        "\r\n",
        "where $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there is sufficient exploration. While a constant $\\alpha$ generally does not guarantee us to reach true convergence, we keep it constant at $\\alpha=0.1$ for this assignment.\r\n",
        "\r\n",
        "## OpenAI Gym\r\n",
        "\r\n",
        "We shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\r\n",
        "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\r\n",
        "The figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\r\n"
      ],
      "metadata": {
        "id": "BQXoOa7LH7e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\r\n",
        "You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Convergence is **not** a constant value, rather a stable plateau with some noise. Take $\\gamma=0.95$. You can refer to the Q-learning (frozen lake) Jupyter notebook shown in class, uploaded on Canvas. Hint: start with a small learning rate.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4a)** Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. Hint: compare values obtained using value iteration and Q-learning.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4b)** What is the importance of exploration in RL? Explain with an example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5"
      ],
      "metadata": {
        "id": "UWUqaN60H7e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5a)** Give a summary of how a decision tree works and how it extends to random forests.\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# References\n",
        "Primer/text based on the following references:\n",
        "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
        "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
      ],
      "metadata": {
        "id": "Wev-_UhcH7e8"
      }
    }
  ]
}