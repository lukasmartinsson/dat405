{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Assignment4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.11 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "ad4bdff1320f12d81f077364a0ed0b61c72bf243b701352d6f35920f5f96452c"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DAT405 Introduction to Data Science and AI \r\n",
        "## 2021-2022, Reading Period 1\r\n",
        "## Assignment 4: Spam classification using Naïve Bayes \r\n",
        "There will be an overall grade for this assignment. To get a pass grade (grade 5), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well. \r\n",
        "\r\n",
        "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \r\n",
        "Hints:\r\n",
        "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \r\n",
        "\r\n",
        "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \r\n",
        "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \r\n",
        "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \r\n",
        "-\thard-ham: non-spam messages more difficult to differentiate \r\n",
        "-\tspam: spam messages \r\n",
        "\r\n",
        "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use 7zip (https://www.7-zip.org/download.html) to decompress the data.\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "-sTsDfIVKsmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "%%script false --no-raise-error\r\n",
        "\r\n",
        "#Download and extract data\r\n",
        "!mkdir datasets\r\n",
        "%cd datasets\r\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\r\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2 \r\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2 \r\n",
        "!tar -xjf 20021010_easy_ham.tar.bz2 \r\n",
        "!tar -xjf 20021010_hard_ham.tar.bz2\r\n",
        "!tar -xjf 20021010_spam.tar.bz2\r\n",
        "%cd .."
      ],
      "outputs": [],
      "metadata": {
        "id": "Wa37fBwRF-xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
      ],
      "metadata": {
        "id": "tdH1XTepLjZ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!ls -lah"
      ],
      "outputs": [],
      "metadata": {
        "id": "A53Gw00fBLG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Preprocessing: \r\n",
        "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. "
      ],
      "metadata": {
        "id": "DGlWPVnSNzT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import os"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def extract_emails(directories, class_name):\r\n",
        "  rows = []\r\n",
        "  for directory in directories:\r\n",
        "    for file in os.listdir(directory):\r\n",
        "      with open(os.path.join(directory,file), encoding='latin-1') as f:\r\n",
        "        if f is not None:\r\n",
        "          rows.append({'email': file, 'content': f.read(), 'class': class_name})\r\n",
        "  return pd.DataFrame(rows)\r\n",
        "\r\n",
        "# extract emails and add class information\r\n",
        "df_easy_ham = extract_emails(['./datasets/easy_ham'], 'ham')\r\n",
        "df_hard_ham = extract_emails(['./datasets/hard_ham'], 'ham')\r\n",
        "df_spam = extract_emails(['./datasets/spam'], 'spam')\r\n",
        "\r\n",
        "# join the two dataframes\r\n",
        "df_combined = pd.concat([df_easy_ham, df_hard_ham, df_spam])\r\n",
        "\r\n",
        "df_combined.sample(5)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# pre-processing code here\r\n",
        "X = df_combined['content']\r\n",
        "y = df_combined['class']\r\n",
        "\r\n",
        "# split dataset\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\r\n",
        "\r\n",
        "# print shapes\r\n",
        "print('df_combined:', df_combined.shape)\r\n",
        "print('X_train:', X_train.shape)\r\n",
        "print('X_test:', X_test.shape)\r\n",
        "print('y_train:', y_train.shape)\r\n",
        "print('y_test:', y_test.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "J2sllUWXKblD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write a Python program that: \r\n",
        "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \r\n",
        "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\r\n",
        "- Multinomial Naive Bayes  \r\n",
        "- Bernoulli Naive Bayes. \r\n",
        "\r\n",
        "Please inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "id": "mnbrbI0_OKCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multinomial Naive Bayes**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\r\n",
        "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\r\n",
        "\r\n",
        "def compare_naive_bayes(X_train, X_test, y_train, y_test, cv=None):\r\n",
        "  # instantiate a Count Vectorizer and fit for training data\r\n",
        "  if cv is None:\r\n",
        "    cv = CountVectorizer()\r\n",
        "\r\n",
        "  X_train_vector = cv.fit_transform(X_train)\r\n",
        "  X_test_vector = cv.transform(X_test)\r\n",
        "\r\n",
        "  # fit models using training vector\r\n",
        "  multinomial_naive_bayes = MultinomialNB().fit(X_train_vector, y_train)\r\n",
        "  bernoulli_naive_bayes = BernoulliNB().fit(X_train_vector, y_train)\r\n",
        "\r\n",
        "  # make predictions on the test data\r\n",
        "  y_pred_multinomial = multinomial_naive_bayes.predict(X_test_vector)\r\n",
        "  y_pred_bernoulli = bernoulli_naive_bayes.predict(X_test_vector)\r\n",
        "\r\n",
        "  # plot confusion matrices\r\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20,10))\r\n",
        "  plot_confusion_matrix(multinomial_naive_bayes, X_test_vector, y_test, ax=ax1, colorbar=None)\r\n",
        "  ax1.set_title(\"Confusion Matrix Multinomial Naive Bayes\")\r\n",
        "  plot_confusion_matrix(bernoulli_naive_bayes, X_test_vector, y_test, ax=ax2, colorbar=True)\r\n",
        "  ax2.set_title('Confusion Matrix Bernoulli Naive Bayes')\r\n",
        "  # cf = confusion_matrix(y_test, bernoulli_naive_bayes.predict(X_test_vector))\r\n",
        "  # plt.colorbar(cf, ax=ax3)\r\n",
        "\r\n",
        "compare_naive_bayes(X_train, X_test, y_train, y_test), "
      ],
      "outputs": [],
      "metadata": {
        "id": "MJERHSCcGNaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the confusion matrices show, the **Multinomial Naive Bayes** model outperforms the **Bernoulli Naive Bayes** model as it suffers from way less misclassifications, and consequently classifies new emails with a higher accuracy. Interestingly enough, both models seem to correctly classify \"ham\" or non-spam mail with similar accuracy. It's the false negative case, when the model classifies a spam mail as \"ham\", that separates the two models, with the **Multinomial Naive Bayes** model being more accurate here."
      ],
      "metadata": {
        "id": "nI1bPDCvQxen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Run your program on \r\n",
        "-\tSpam versus easy-ham \r\n"
      ],
      "metadata": {
        "id": "wDFS3uFFUcS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def spam_vs_easy_ham(cv=None):\r\n",
        "  spam_vs_easy_ham = pd.concat([df_spam, df_easy_ham])\r\n",
        "  X = spam_vs_easy_ham['content']\r\n",
        "  y = spam_vs_easy_ham['class']\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\r\n",
        "  compare_naive_bayes(X_train, X_test, y_train, y_test, cv=cv)\r\n",
        "\r\n",
        "spam_vs_easy_ham()"
      ],
      "outputs": [],
      "metadata": {
        "id": "gool_zb8Qzzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\tSpam versus hard-ham."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def spam_vs_hard_ham(cv=None):\r\n",
        "  spam_vs_hard_ham = pd.concat([df_spam, df_hard_ham])\r\n",
        "  X = spam_vs_hard_ham['content']\r\n",
        "  y = spam_vs_hard_ham['class']\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)\r\n",
        "  compare_naive_bayes(X_train, X_test, y_train, y_test, cv=cv)\r\n",
        "\r\n",
        "spam_vs_hard_ham()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \r\n",
        "\r\n",
        "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. \r\n",
        "\r\n",
        "Discarding common and uninformative words removes what could be considered as noise in the data, which might help improve the accuracy of the two classification models. For instance, looking at the 20 most common words below, one notice that most of these, like `com`, `the`, `to` and `from`are either very uninformative or used in the email headers and thus likely to be irrelevant for deciding if it's spam or ham.\r\n"
      ],
      "metadata": {
        "id": "TkfQWBB4UhYd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_top_n_words(corpus, n=None):\r\n",
        "    cv = CountVectorizer().fit(corpus)\r\n",
        "    word_vector = cv.transform(corpus)\r\n",
        "    word_sums = word_vector.sum(axis=0) \r\n",
        "    words_freq = [(word, word_sums[0, idx]) for word, idx in cv.vocabulary_.items()]\r\n",
        "    words_freq = sorted(words_freq, key = lambda word: word[1], reverse=True)\r\n",
        "    return words_freq[:n]\r\n",
        "\r\n",
        "top_common_words = get_top_n_words(df_combined['content'])\r\n",
        "top_common_words"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\r\n",
        "\r\n",
        "You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a custom word filter**\r\n",
        "\r\n",
        "Here one needs to specify the number of words to filter out, in order of word frequency."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "custom_stop_words = [word[0] for word in top_common_words[:20]]\r\n",
        "print(f'filtered out words: {custom_stop_words}')\r\n",
        "\r\n",
        "# generate word count vectorizer with custom in filter\r\n",
        "cv_custom_filter = CountVectorizer(stop_words=custom_stop_words)\r\n",
        "\r\n",
        "spam_vs_easy_ham(cv=cv_custom_filter)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qt7ELzEqUfas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing it with `spam vs hard ham` dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "spam_vs_hard_ham(cv=cv_custom_filter)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using SKlearn's built in filtering**\r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# generate word count vectorizer configured with built in filter\r\n",
        "cv_built_in_filter = CountVectorizer(max_df=0.9, stop_words='english')\r\n",
        "\r\n",
        "spam_vs_easy_ham(cv=cv_built_in_filter)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing it with `spam vs hard ham` dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "spam_vs_hard_ham(cv=cv_built_in_filter)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in the question, one can either let `sklearn` filter out common words using  the built-in stop word list for English, or one can manually specify this list. In the documentation, the authors of the `sklern` library mention there are several known issues with using the built in stop word list and that one should consider an alternative. A reason to manually specify the stop words is that this allows for experimenting with different configuration which might fit the particular dataset a lot better. On the other hand this might also increase the risk of overfitting the models with the consequence of poor accuracy when presented with new emails.\r\n",
        "\r\n",
        "We decided to go with the built in method since it's been developed and tested over many years by more experienced engineers, as well as to avoid overfitting"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Eeking out further performance\r\n",
        "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \r\n",
        "-\tDoes the result improve from 3 and 4? \r\n",
        "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \r\n",
        "- What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \r\n",
        "\r\n",
        "Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\r\n",
        "- What does this parameter mean?\r\n",
        "- How does this alter the predictions? Discuss why or why not."
      ],
      "metadata": {
        "id": "zcyVfOZFU4F_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "s_nyGug9U4f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What to report and how to hand in.\n",
        "\n",
        "- You will need to clearly report all results in the notebook in a clear and appropriate way, either using plots or code output (f.x. \"print statements\"). \n",
        "- The notebook must be reproducible, that means, we must be able to use the `Run all` function from the `Runtime` menu and reproduce all your results. **Please check this before handing in.** \n",
        "- Save the notebook and share a link to the notebook (Press share in upper left corner, and use `Get link` option. **Please make sure to allow all with the link to open and edit.**\n",
        "- Edits made after submission deadline will be ignored, graders will recover the last saved version before deadline from the revisions history.\n",
        "- **Please make sure all cells are executed and all the output is clearly readable/visible to anybody opening the notebook.**"
      ],
      "metadata": {
        "id": "ND6FKoexVAhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "8bI3z_spVacz"
      }
    }
  ]
}